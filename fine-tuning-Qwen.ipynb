{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOMIPK0wYcd2sEcW5l+llJ+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ea6221e7ad0e4403bb12de5670cce30a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27196c2ae5334e868de91bb4f10e18ec","IPY_MODEL_3c20bd6bc3cf433f8009f3c7f1e01d33","IPY_MODEL_8570c653db6641649ce8127e342ddae0"],"layout":"IPY_MODEL_a3b0bbeac1cd41d18046d519129c889b"}},"27196c2ae5334e868de91bb4f10e18ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d1c3d3a5a8a445fa7cb95d1e804182c","placeholder":"​","style":"IPY_MODEL_173a6ef3839e4abf8bdfeb13cdc49cb1","value":"config.json: 100%"}},"3c20bd6bc3cf433f8009f3c7f1e01d33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_249dcdc38ed04e09b5405eb434b10cb6","max":660,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3ebf038284a4e2f82d3771952ffec14","value":660}},"8570c653db6641649ce8127e342ddae0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8953546b3b424b0d9a18ddd5f586c5d7","placeholder":"​","style":"IPY_MODEL_e083cebee05f425989a0303f156a196b","value":" 660/660 [00:00&lt;00:00, 15.2kB/s]"}},"a3b0bbeac1cd41d18046d519129c889b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d1c3d3a5a8a445fa7cb95d1e804182c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"173a6ef3839e4abf8bdfeb13cdc49cb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"249dcdc38ed04e09b5405eb434b10cb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3ebf038284a4e2f82d3771952ffec14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8953546b3b424b0d9a18ddd5f586c5d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e083cebee05f425989a0303f156a196b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e73e9058fc34186b62e0047f5225d43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_843eb284feea4254b88a628d007f56a6","IPY_MODEL_06ec36c6b8f54e69843dfb2e4a3dce8d","IPY_MODEL_5aec1132043d4189952d26e0c4ce63ce"],"layout":"IPY_MODEL_ba43ec0f141c4e5686f8fc7ba9e18df4"}},"843eb284feea4254b88a628d007f56a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_951c46e2a9ff46eebbf97ebea18e5d14","placeholder":"​","style":"IPY_MODEL_9c8ea27fcfc34915a54cf83722a560b4","value":"model.safetensors: 100%"}},"06ec36c6b8f54e69843dfb2e4a3dce8d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dac9d1662f743538c2c2c786e98d646","max":3087467144,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bfd15bbdf87b4db08b6011bb3be7dfee","value":3087467144}},"5aec1132043d4189952d26e0c4ce63ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fedc64eff2c342948c265529536846ff","placeholder":"​","style":"IPY_MODEL_becdf82aafc340e2bf86a5f42ec4d77d","value":" 3.09G/3.09G [00:36&lt;00:00, 243MB/s]"}},"ba43ec0f141c4e5686f8fc7ba9e18df4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"951c46e2a9ff46eebbf97ebea18e5d14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c8ea27fcfc34915a54cf83722a560b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dac9d1662f743538c2c2c786e98d646":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd15bbdf87b4db08b6011bb3be7dfee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fedc64eff2c342948c265529536846ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"becdf82aafc340e2bf86a5f42ec4d77d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d028ffcf39d4543913963cc53a8e0e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0caa065420bd43ecbf4324f826c540dd","IPY_MODEL_d123b09ce77e49839dda70ecd99d8e37","IPY_MODEL_e306fab2fcf34c3b99fd65ea887e1818"],"layout":"IPY_MODEL_7ec7b0b6636b41c18a62e15e1dbc91cd"}},"0caa065420bd43ecbf4324f826c540dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00a06bb76f2d4532ace348ef43fc0896","placeholder":"​","style":"IPY_MODEL_09a6679daca94b339428475dd55c3e03","value":"generation_config.json: 100%"}},"d123b09ce77e49839dda70ecd99d8e37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18492eb43ec243b59762bd00d04bd4d8","max":242,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a69025dd45d746d9a6edfd31b6b9a956","value":242}},"e306fab2fcf34c3b99fd65ea887e1818":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c87a87bbda084bd2a6e3ce70853be7f2","placeholder":"​","style":"IPY_MODEL_d9dd421d46db4c3791086148c381bf1d","value":" 242/242 [00:00&lt;00:00, 30.1kB/s]"}},"7ec7b0b6636b41c18a62e15e1dbc91cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00a06bb76f2d4532ace348ef43fc0896":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09a6679daca94b339428475dd55c3e03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18492eb43ec243b59762bd00d04bd4d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a69025dd45d746d9a6edfd31b6b9a956":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c87a87bbda084bd2a6e3ce70853be7f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9dd421d46db4c3791086148c381bf1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"815084211b044da9886ee6d1287def14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61fcd95d9dcb4ab29dcc5ca227e98b3f","IPY_MODEL_4e5d6a0f327146a3aac1d8264f423873","IPY_MODEL_1c1eb59536dc49a9b34c532534f3f90a"],"layout":"IPY_MODEL_ee19783c7854453db392218e5e377b83"}},"61fcd95d9dcb4ab29dcc5ca227e98b3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0615da7bbad64e62a831095697776fa9","placeholder":"​","style":"IPY_MODEL_6bc28a9741e34b8ab7a140f2da6b9193","value":"tokenizer_config.json: "}},"4e5d6a0f327146a3aac1d8264f423873":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36beb6ea11914d088c15672974edef10","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bb3878ed3d14cf790845cf9cf6c26e5","value":1}},"1c1eb59536dc49a9b34c532534f3f90a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e95a8b78094b4b65a19db9ac5f4c4220","placeholder":"​","style":"IPY_MODEL_797979f086624897a23a530f727cec89","value":" 7.30k/? [00:00&lt;00:00, 788kB/s]"}},"ee19783c7854453db392218e5e377b83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0615da7bbad64e62a831095697776fa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bc28a9741e34b8ab7a140f2da6b9193":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36beb6ea11914d088c15672974edef10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6bb3878ed3d14cf790845cf9cf6c26e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e95a8b78094b4b65a19db9ac5f4c4220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"797979f086624897a23a530f727cec89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaa0d58cb5854ee2a111bd6705bcb389":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5b7e18bc05c4965a4264810115bd9c4","IPY_MODEL_e77648c8e52e4367ad544efebb78c0a8","IPY_MODEL_684f7f21f12d4f1fa2643dbce7287104"],"layout":"IPY_MODEL_4bfc24599bc441f992c6a2f697a733c7"}},"a5b7e18bc05c4965a4264810115bd9c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d510fef0bfd418598f01699446c5f96","placeholder":"​","style":"IPY_MODEL_b9783e78668f413fb0f0ba3fe6810ed7","value":"vocab.json: "}},"e77648c8e52e4367ad544efebb78c0a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d505e989468451e852957990792b736","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5f9d7c627a54ac6967e0f45ffa22d65","value":1}},"684f7f21f12d4f1fa2643dbce7287104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3be56811d9644e9dbfa2476629b52c67","placeholder":"​","style":"IPY_MODEL_ae2b5c4bc5a447a2ad8a45efbb2a5976","value":" 2.78M/? [00:00&lt;00:00, 50.0MB/s]"}},"4bfc24599bc441f992c6a2f697a733c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d510fef0bfd418598f01699446c5f96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9783e78668f413fb0f0ba3fe6810ed7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d505e989468451e852957990792b736":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c5f9d7c627a54ac6967e0f45ffa22d65":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3be56811d9644e9dbfa2476629b52c67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae2b5c4bc5a447a2ad8a45efbb2a5976":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d630c7cf1ef84af4b9f5d8e1fea5e037":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73050368f3ce4aa69aee262468b0ce92","IPY_MODEL_16a81735ea164eedbb1efad1a1480c53","IPY_MODEL_f43a51cf247840bca5c11956dc8ec64b"],"layout":"IPY_MODEL_6b92b8d7dd2a44d6bc8e5cef95590cab"}},"73050368f3ce4aa69aee262468b0ce92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0e16c42f25944bc846be316208c6381","placeholder":"​","style":"IPY_MODEL_38202fb0ea23473c9473ceac3be7f5b4","value":"merges.txt: "}},"16a81735ea164eedbb1efad1a1480c53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_210d0d50c82140ae9a15045658bac10c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16914e2c36e6494c8d89ed5f791630ca","value":1}},"f43a51cf247840bca5c11956dc8ec64b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_415d50d5acea4891a78222a1a8a5a198","placeholder":"​","style":"IPY_MODEL_a89b724326bf47e8918618adff066412","value":" 1.67M/? [00:00&lt;00:00, 45.2MB/s]"}},"6b92b8d7dd2a44d6bc8e5cef95590cab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0e16c42f25944bc846be316208c6381":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38202fb0ea23473c9473ceac3be7f5b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"210d0d50c82140ae9a15045658bac10c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"16914e2c36e6494c8d89ed5f791630ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"415d50d5acea4891a78222a1a8a5a198":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a89b724326bf47e8918618adff066412":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df29c96a36a54651a5827094f386f7c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc91282c8be24794af0e017e69cc6c8d","IPY_MODEL_116fe54bd662405192413e195e2bc12f","IPY_MODEL_5e27eee1a9834993be52f1240131a3fa"],"layout":"IPY_MODEL_ccd21f0dc86241aca076114143a737ee"}},"dc91282c8be24794af0e017e69cc6c8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_005cf11513624980ae9c6fb717255fba","placeholder":"​","style":"IPY_MODEL_cf19f575c2d3489e9ce9e5e48800e272","value":"tokenizer.json: "}},"116fe54bd662405192413e195e2bc12f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c661e06faf1743fc9f5af9f78ac9ba06","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3f9a6cfd4984c46abf9221f23d84d72","value":1}},"5e27eee1a9834993be52f1240131a3fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f3727d04e3b4ab5b64d2e335158c11f","placeholder":"​","style":"IPY_MODEL_4812d93fd0ef492d9e865896815f240e","value":" 7.03M/? [00:00&lt;00:00, 81.5MB/s]"}},"ccd21f0dc86241aca076114143a737ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"005cf11513624980ae9c6fb717255fba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf19f575c2d3489e9ce9e5e48800e272":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c661e06faf1743fc9f5af9f78ac9ba06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c3f9a6cfd4984c46abf9221f23d84d72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f3727d04e3b4ab5b64d2e335158c11f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4812d93fd0ef492d9e865896815f240e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KT34z8qwGIl9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767035216119,"user_tz":-180,"elapsed":3197,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"4ae746dd-d59d-49f6-d4af-06c9675e6db8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","\n","hf_token = key=userdata.get('Hug_TOKEN')\n","!hf auth login --token {hf_token}\n","!hf --add-to-git-credential"],"metadata":{"id":"tqv9t2abX3Si","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766867030408,"user_tz":-180,"elapsed":1897,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"bfbf02c4-b828-47b4-9003-3d1aa1f51f24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","The token `llamafactory-push` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `llamafactory-push`\n","usage: hf <command> [<args>]\n","hf: error: unrecognized arguments: --add-to-git-credential\n"]}]},{"cell_type":"code","source":["# @title\n","\n","# This code for converting JSONL into qwen structure (useless in our case)\n","from transformers import AutoTokenizer\n","import json\n","\n","base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","input_file = \"/content/drive/MyDrive/Data/all_MCQ.jsonl\"\n","output_file = \"/content/drive/MyDrive/Data/qwen_formatted_MCQ.jsonl\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","\n","conversations = []\n","with open(input_file, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        if line:\n","            conversations.append(json.loads(line))\n","\n","formatted_conversations = []\n","\n","for conv in conversations:\n","    # If data has \"messages\" key\n","    if \"messages\" in conv:\n","        messages = conv[\"messages\"]\n","    # If data is already the messages list\n","    elif isinstance(conv, list):\n","        messages = conv\n","    # If data has different structure\n","    else:\n","        print(f\"Unexpected format: {conv.keys() if isinstance(conv, dict) else type(conv)}\")\n","        continue\n","\n","    try:\n","        formatted_text = tokenizer.apply_chat_template(\n","            messages,\n","            tokenize=False,\n","            add_generation_prompt=True\n","        )\n","\n","        formatted_conversations.append({\n","            \"text\": formatted_text,\n","            \"original\": conv\n","        })\n","\n","    except Exception as e:\n","        print(f\"Error processing: {e}\")\n","        print(f\"Data: {conv}\")\n","        continue\n","\n","# Save the formatted data\n","with open(output_file, 'w', encoding='utf-8') as f:\n","    for conv in formatted_conversations:\n","        f.write(json.dumps(conv, ensure_ascii=False) + '\\n')\n","\n","print(f\"Successfully formatted {len(formatted_conversations)} out of {len(conversations)} conversations\")"],"metadata":{"id":"Dv9gdOtLMNds","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Convert to alpaca format"],"metadata":{"id":"CkY53_hyFaTi"}},{"cell_type":"code","source":["# @title\n","import json\n","\n","prompt = \"\\n\".join([\n","            \"You are a quiz parser. Output JSON for questions, \\\"SKIP\\\" for non-questions.\",\n","            \"\",\n","            \"Format: {\\\"question\\\": \\\"text\\\", \\\"answers\\\": {\\\"1\\\": \\\"opt1\\\", \\\"2\\\": \\\"opt2\\\"}, \\\"correct\\\": \\\"1\\\"}\",\n","            \"\",\n","            \"Key Rules:\",\n","            \"1. Clean: remove numbering (Q1:, 1., etc.) and formatting markers\",\n","            \"2. Answers: always use keys \\\"1\\\", \\\"2\\\", \\\"3\\\"\",\n","            \"3. Correct: find from bold/**text**, [highlight], ✅, [colored], or trailing indicators\",\n","            \"4. CRITICAL: All options same format → \\\"0\\\"\",\n","            \"5. CRITICAL: 2+ marked OR no marking → \\\"0\\\"\",\n","            \"6. Output ONLY JSON or \\\"SKIP\\\"\",\n","            \"\",\n","            \"Examples:\",\n","            \"Input: Q1: What is 2+2? **A) 4** B) 5\",\n","            \"Output: {\\\"question\\\": \\\"What is 2+2?\\\", \\\"answers\\\": {\\\"1\\\": \\\"4\\\", \\\"2\\\": \\\"5\\\"}, \\\"correct\\\": \\\"1\\\"}\",\n","            \"\",\n","            \"Input: Capital of Spain? a) Madrid b) Barcelona c) Valencia. Answer: a\",\n","            \"Output: {\\\"question\\\": \\\"Capital of Spain?\\\", \\\"answers\\\": {\\\"1\\\": \\\"Madrid\\\", \\\"2\\\": \\\"Barcelona\\\", \\\"3\\\": \\\"Valencia\\\"}, \\\"correct\\\": \\\"1\\\"}\",\n","            \"\",\n","            \"Input: Which is correct? **a) X** **b) Y** **c) Z**\",\n","            \"Output: {\\\"question\\\": \\\"Which is correct?\\\", \\\"answers\\\": {\\\"1\\\": \\\"X\\\", \\\"2\\\": \\\"Y\\\", \\\"3\\\": \\\"Z\\\"}, \\\"correct\\\": \\\"0\\\"}\"\n","        ])\n","TASK = \"\\n\".join([\n","            \"Convert the following multiple-choice question into JSON.\",\n","            \"If it is not a valid question, output SKIP.\",\n","        ])\n","\n","def convert_to_alpaca(input_file, output_file):\n","    with open(input_file, 'r', encoding='utf-8') as f_in, \\\n","         open(output_file, 'w', encoding='utf-8') as f_out:\n","\n","        for line in f_in:\n","            data = json.loads(line)\n","            messages = data.get(\"messages\", [])\n","\n","            user = \"\"\n","            assistant = \"\"\n","\n","            for msg in messages:\n","                role = msg.get(\"role\")\n","                content = msg.get(\"content\")\n","\n","                if role == \"user\":\n","                    user = content\n","                elif role == \"assistant\":\n","                    assistant = content\n","\n","            alpaca_row = {\n","                \"system\": prompt,\n","                \"instruction\": TASK,\n","                \"input\": user,\n","                \"output\": assistant,\n","                \"history\": []\n","            }\n","\n","            f_out.write(json.dumps(alpaca_row, ensure_ascii=False) + \"\\n\")\n","\n","    print(f\"✅ Converted to Alpaca format: {output_file}\")\n","\n","# Convert your files\n","convert_to_alpaca(\n","    '/content/drive/MyDrive/data/all_MCQ.jsonl',\n","    '/content/drive/MyDrive/data/all_MCQs.jsonl'\n",")\n","\n","convert_to_alpaca(\n","    '/content/drive/MyDrive/data/evaluation.jsonl',\n","    '/content/drive/MyDrive/data/evaluations.jsonl'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jR5Ry_nIdWoM","executionInfo":{"status":"ok","timestamp":1766397733433,"user_tz":-180,"elapsed":600,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"1bdce1d0-a66d-4bfc-dbdf-e10c41e9c059","cellView":"form","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Converted to Alpaca format: /content/drive/MyDrive/data/all_MCQs.jsonl\n","✅ Converted to Alpaca format: /content/drive/MyDrive/data/evaluations.jsonl\n"]}]},{"cell_type":"markdown","source":["## Fine-tuning"],"metadata":{"id":"x8DH4yhFt7w3"}},{"cell_type":"code","source":["!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n","!cd LLaMA-Factory && pip install -e ."],"metadata":{"collapsed":true,"id":"5NLulzYuCH0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U \"numpy<2.0\" --quiet\n","\n","!python -m pip install --upgrade pip setuptools wheel\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install transformers accelerate datasets peft trl\n","!pip install -e LLaMA-Factory/. --no-deps\n","!pip install gradio fire tyro shtab\n","!pip install -U \"datasets>=2.16.0,<4.1.0\"\n","!pip install -U \"transformers>=4.49.0,<=4.57.1\""],"metadata":{"collapsed":true,"id":"fKBiFUrxG3SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U bitsandbytes"],"metadata":{"id":"Ve_1MCao3NH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLaMA-Factory\n","# Go to /content/LLaMA-Factory/data/dataset_info.json and add the following:\n","\"\"\"\n","  \"quiz_parser_train\": {\n","    \"file_name\": \"/content/drive/MyDrive/data/all_MCQs.jsonl\",\n","    \"columns\": {\n","        \"prompt\": \"instruction\",\n","        \"query\": \"input\",\n","        \"response\": \"output\",\n","        \"system\": \"system\",\n","        \"history\": \"history\"\n","    }\n","  },\n","  \"quiz_parser_eval\": {\n","    \"file_name\": \"/content/drive/MyDrive/data/evaluations.jsonl\",\n","    \"columns\": {\n","        \"prompt\": \"instruction\",\n","        \"query\": \"input\",\n","        \"response\": \"output\",\n","        \"system\": \"system\",\n","        \"history\": \"history\"\n","    }\n","  }\n","\"\"\""],"metadata":{"id":"AlOkCKaGBxyp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf ~/.cache/huggingface"],"metadata":{"id":"in0dHa2gulxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global credential.helper store\n","!hf auth logout\n","!hf auth login --add-to-git-credential"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"IWgckLmxtKxj","executionInfo":{"status":"ok","timestamp":1766867070910,"user_tz":-180,"elapsed":26829,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"4f852a3d-5ebf-42e0-bdfa-edbd3388c25c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Not logged in!\n","\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) Y\n","Token is valid (permission: fineGrained).\n","The token `llamafactory-push` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved in your configured git credential helpers (store).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `llamafactory-push`\n"]}]},{"cell_type":"code","source":["%%writefile /content/LLaMA-Factory/examples/train_lora/quiz_finetune.yaml\n","\n","### Model\n","model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n","trust_remote_code: true\n","\n","### Method\n","stage: sft\n","do_train: true\n","do_eval: true\n","finetuning_type: lora\n","lora_rank: 8\n","lora_alpha: 16\n","lora_dropout: 0.05\n","lora_target: all\n","\n","### Dataset\n","dataset: quiz_parser_train\n","eval_dataset: quiz_parser_eval\n","template: qwen\n","cutoff_len: 2048\n","overwrite_cache: true\n","preprocessing_num_workers: 4\n","\n","### Training (T4 Optimized)\n","per_device_train_batch_size: 1\n","gradient_accumulation_steps: 16\n","learning_rate: 1.0e-4\n","num_train_epochs: 3.0\n","#max_steps: -1\n","lr_scheduler_type: cosine\n","warmup_ratio: 0.1\n","optim: adamw_8bit\n","fp16: true\n","gradient_checkpointing: true\n","logging_steps: 10\n","save_steps: 200\n","save_total_limit: 2\n","\n","### Evaluation\n","#val_size: 0.1\n","per_device_eval_batch_size: 1\n","eval_strategy: steps\n","eval_steps: 100\n","\n","### Output\n","output_dir: /content/drive/MyDrive/llm-finetuning/quiz_parser\n","overwrite_output_dir: true\n","\n","### hf\n","push_to_hub: true\n","hub_model_id: \"Fadhl0/quiz-parser-qwen-1.5b\"\n","hub_private_repo: true\n","hub_strategy: checkpoint\n","\n","### System\n","ddp_timeout: 180000000\n","dataloader_num_workers: 2\n","report_to: \"none\""],"metadata":{"id":"Pa6xfX9dYqCm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766403727423,"user_tz":-180,"elapsed":6,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"7bfd22b8-1b1d-43bd-cc79-fb485791bfc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/LLaMA-Factory/examples/train_lora/quiz_finetune.yaml\n"]}]},{"cell_type":"code","source":["!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/quiz_finetune.yaml"],"metadata":{"id":"Ieq32YWKdUdC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766407621171,"user_tz":-180,"elapsed":3881644,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"2e615806-beb9-40e3-8cf4-975ffc1350d1","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-12-22 11:42:33.670692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1766403753.709342   25457 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1766403753.720085   25457 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1766403753.745167   25457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1766403753.745198   25457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1766403753.745202   25457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1766403753.745206   25457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-12-22 11:42:33.752349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","/usr/local/lib/python3.12/dist-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n","  import pkg_resources\n","[INFO|2025-12-22 11:42:51] llamafactory.hparams.parser:465 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n","tokenizer_config.json: 7.30kB [00:00, 2.01MB/s]\n","vocab.json: 2.78MB [00:00, 10.4MB/s]\n","merges.txt: 1.67MB [00:00, 12.2MB/s]\n","tokenizer.json: 7.03MB [00:00, 24.5MB/s]\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:53,579 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:53,579 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:53,579 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:53,579 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:53,579 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:53,579 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:53,579 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2364] 2025-12-22 11:42:53,883 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","config.json: 100% 660/660 [00:00<00:00, 5.62MB/s]\n","[INFO|configuration_utils.py:765] 2025-12-22 11:42:54,578 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 11:42:54,581 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:54,773 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:54,773 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:54,773 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:54,774 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:54,774 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:54,774 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2095] 2025-12-22 11:42:54,774 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2364] 2025-12-22 11:42:55,127 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-12-22 11:42:55] llamafactory.data.loader:143 >> Loading dataset /content/drive/MyDrive/data/all_MCQs.jsonl...\n","Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","WARNING:datasets.builder:Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","Generating train split: 2197 examples [00:00, 45824.91 examples/s]\n","Converting format of dataset (num_proc=4): 100% 2197/2197 [00:00<00:00, 3863.63 examples/s]\n","[INFO|2025-12-22 11:42:56] llamafactory.data.loader:143 >> Loading dataset /content/drive/MyDrive/data/evaluations.jsonl...\n","Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","WARNING:datasets.builder:Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","Generating train split: 251 examples [00:00, 33866.38 examples/s]\n","Converting format of dataset (num_proc=4): 100% 251/251 [00:00<00:00, 716.94 examples/s]\n","Running tokenizer on dataset (num_proc=4): 100% 2197/2197 [00:05<00:00, 417.25 examples/s]\n","training example:\n","input_ids:\n","[151644, 8948, 198, 2610, 525, 264, 27124, 6729, 13, 9258, 4718, 369, 4755, 11, 330, 91799, 1, 369, 2477, 12, 17348, 382, 4061, 25, 5212, 7841, 788, 330, 1318, 497, 330, 24710, 788, 5212, 16, 788, 330, 2912, 16, 497, 330, 17, 788, 330, 2912, 17, 14345, 330, 19928, 788, 330, 16, 63159, 1592, 22847, 510, 16, 13, 9590, 25, 4057, 78262, 320, 48, 16, 41239, 220, 16, 2572, 4992, 6138, 323, 36566, 23962, 198, 17, 13, 37243, 25, 2677, 990, 6894, 330, 16, 497, 330, 17, 497, 330, 18, 698, 18, 13, 39970, 25, 1477, 504, 13939, 3663, 1318, 97219, 508, 35198, 1125, 25521, 227, 11, 508, 2074, 3018, 1125, 476, 27748, 33724, 198, 19, 13, 12617, 46817, 25, 2009, 2606, 1852, 3561, 11397, 330, 15, 698, 20, 13, 12617, 46817, 25, 220, 17, 10, 12864, 2726, 902, 34924, 11397, 330, 15, 698, 21, 13, 9258, 26687, 4718, 476, 330, 91799, 1837, 40381, 510, 2505, 25, 1207, 16, 25, 3555, 374, 220, 17, 10, 17, 30, 3070, 32, 8, 220, 19, 334, 425, 8, 220, 20, 198, 5097, 25, 5212, 7841, 788, 330, 3838, 374, 220, 17, 10, 17, 31011, 330, 24710, 788, 5212, 16, 788, 330, 19, 497, 330, 17, 788, 330, 20, 14345, 330, 19928, 788, 330, 16, 63159, 2505, 25, 18374, 315, 17689, 30, 264, 8, 24081, 293, 8, 26937, 272, 8, 60249, 13, 21806, 25, 264, 198, 5097, 25, 5212, 7841, 788, 330, 63593, 315, 17689, 31011, 330, 24710, 788, 5212, 16, 788, 330, 37036, 1869, 497, 330, 17, 788, 330, 3428, 22365, 497, 330, 18, 788, 330, 2208, 8792, 14345, 330, 19928, 788, 330, 16, 63159, 2505, 25, 15920, 374, 4396, 30, 3070, 64, 8, 1599, 334, 3070, 65, 8, 809, 334, 3070, 66, 8, 1863, 1019, 5097, 25, 5212, 7841, 788, 330, 23085, 374, 4396, 31011, 330, 24710, 788, 5212, 16, 788, 330, 55, 497, 330, 17, 788, 330, 56, 497, 330, 18, 788, 330, 57, 14345, 330, 19928, 788, 330, 15, 9207, 151645, 198, 151644, 872, 198, 12012, 279, 2701, 5248, 62626, 3405, 1119, 4718, 624, 2679, 432, 374, 537, 264, 2697, 3405, 11, 2550, 81404, 624, 124009, 128321, 53479, 125669, 69423, 27846, 124074, 1, 124227, 73274, 129383, 20064, 123920, 85153, 124176, 39423, 124138, 27846, 140683, 1, 128332, 198, 69682, 8, 4102, 130283, 4102, 12961, 11071, 125492, 70604, 4102, 125621, 11798, 124495, 624, 144247, 21360, 8, 4102, 130283, 4102, 85153, 32790, 127471, 124138, 4102, 126196, 4102, 128286, 4102, 77273, 4102, 43982, 21360, 65398, 129080, 624, 33090, 8, 4102, 94957, 73771, 124223, 73771, 31073, 4102, 124476, 124514, 70604, 13, 151645, 198, 151644, 77091, 198, 4913, 7841, 788, 330, 124009, 128321, 53479, 125669, 69423, 27846, 124074, 2105, 124227, 73274, 129383, 20064, 123920, 85153, 124176, 39423, 124138, 27846, 140683, 2105, 128332, 497, 330, 24710, 788, 5212, 16, 788, 330, 69682, 8, 4102, 130283, 4102, 12961, 11071, 125492, 70604, 4102, 125621, 11798, 124495, 10465, 330, 17, 788, 330, 21360, 8, 4102, 130283, 4102, 85153, 32790, 127471, 124138, 4102, 126196, 4102, 128286, 4102, 77273, 4102, 43982, 21360, 65398, 129080, 10465, 330, 18, 788, 330, 33090, 8, 4102, 94957, 73771, 124223, 73771, 31073, 4102, 124476, 124514, 70604, 1189, 2137, 330, 19928, 788, 330, 17, 9207, 151645, 198]\n","inputs:\n","<|im_start|>system\n","You are a quiz parser. Output JSON for questions, \"SKIP\" for non-questions.\n","\n","Format: {\"question\": \"text\", \"answers\": {\"1\": \"opt1\", \"2\": \"opt2\"}, \"correct\": \"1\"}\n","\n","Key Rules:\n","1. Clean: remove numbering (Q1:, 1., etc.) and formatting markers\n","2. Answers: always use keys \"1\", \"2\", \"3\"\n","3. Correct: find from bold/**text**, [highlight], ✅, [colored], or trailing indicators\n","4. CRITICAL: All options same format → \"0\"\n","5. CRITICAL: 2+ marked OR no marking → \"0\"\n","6. Output ONLY JSON or \"SKIP\"\n","\n","Examples:\n","Input: Q1: What is 2+2? **A) 4** B) 5\n","Output: {\"question\": \"What is 2+2?\", \"answers\": {\"1\": \"4\", \"2\": \"5\"}, \"correct\": \"1\"}\n","\n","Input: Capital of Spain? a) Madrid b) Barcelona c) Valencia. Answer: a\n","Output: {\"question\": \"Capital of Spain?\", \"answers\": {\"1\": \"Madrid\", \"2\": \"Barcelona\", \"3\": \"Valencia\"}, \"correct\": \"1\"}\n","\n","Input: Which is correct? **a) X** **b) Y** **c) Z**\n","Output: {\"question\": \"Which is correct?\", \"answers\": {\"1\": \"X\", \"2\": \"Y\", \"3\": \"Z\"}, \"correct\": \"0\"}<|im_end|>\n","<|im_start|>user\n","Convert the following multiple-choice question into JSON.\n","If it is not a valid question, output SKIP.\n","ما هو المقصود بـ\"لم يلبسوا إيمانهم بظلم\"؟\n","أ)  عدم  ارتكاب  الذنوب.\n","✅ب)  عدم  إشراكهم  مع  الله  في  عبادتهم.\n","ج)  التّمسّك  بالكتاب.<|im_end|>\n","<|im_start|>assistant\n","{\"question\": \"ما هو المقصود بـ\\\"لم يلبسوا إيمانهم بظلم\\\"؟\", \"answers\": {\"1\": \"أ)  عدم  ارتكاب  الذنوب.\", \"2\": \"ب)  عدم  إشراكهم  مع  الله  في  عبادتهم.\", \"3\": \"ج)  التّمسّك  بالكتاب.\"}, \"correct\": \"2\"}<|im_end|>\n","\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 7841, 788, 330, 124009, 128321, 53479, 125669, 69423, 27846, 124074, 2105, 124227, 73274, 129383, 20064, 123920, 85153, 124176, 39423, 124138, 27846, 140683, 2105, 128332, 497, 330, 24710, 788, 5212, 16, 788, 330, 69682, 8, 4102, 130283, 4102, 12961, 11071, 125492, 70604, 4102, 125621, 11798, 124495, 10465, 330, 17, 788, 330, 21360, 8, 4102, 130283, 4102, 85153, 32790, 127471, 124138, 4102, 126196, 4102, 128286, 4102, 77273, 4102, 43982, 21360, 65398, 129080, 10465, 330, 18, 788, 330, 33090, 8, 4102, 94957, 73771, 124223, 73771, 31073, 4102, 124476, 124514, 70604, 1189, 2137, 330, 19928, 788, 330, 17, 9207, 151645, 198]\n","labels:\n","{\"question\": \"ما هو المقصود بـ\\\"لم يلبسوا إيمانهم بظلم\\\"؟\", \"answers\": {\"1\": \"أ)  عدم  ارتكاب  الذنوب.\", \"2\": \"ب)  عدم  إشراكهم  مع  الله  في  عبادتهم.\", \"3\": \"ج)  التّمسّك  بالكتاب.\"}, \"correct\": \"2\"}<|im_end|>\n","\n","Running tokenizer on dataset (num_proc=4): 100% 251/251 [00:04<00:00, 59.95 examples/s]\n","eval example:\n","input_ids:\n","[151644, 8948, 198, 2610, 525, 264, 27124, 6729, 13, 9258, 4718, 369, 4755, 11, 330, 91799, 1, 369, 2477, 12, 17348, 382, 4061, 25, 5212, 7841, 788, 330, 1318, 497, 330, 24710, 788, 5212, 16, 788, 330, 2912, 16, 497, 330, 17, 788, 330, 2912, 17, 14345, 330, 19928, 788, 330, 16, 63159, 1592, 22847, 510, 16, 13, 9590, 25, 4057, 78262, 320, 48, 16, 41239, 220, 16, 2572, 4992, 6138, 323, 36566, 23962, 198, 17, 13, 37243, 25, 2677, 990, 6894, 330, 16, 497, 330, 17, 497, 330, 18, 698, 18, 13, 39970, 25, 1477, 504, 13939, 3663, 1318, 97219, 508, 35198, 1125, 25521, 227, 11, 508, 2074, 3018, 1125, 476, 27748, 33724, 198, 19, 13, 12617, 46817, 25, 2009, 2606, 1852, 3561, 11397, 330, 15, 698, 20, 13, 12617, 46817, 25, 220, 17, 10, 12864, 2726, 902, 34924, 11397, 330, 15, 698, 21, 13, 9258, 26687, 4718, 476, 330, 91799, 1837, 40381, 510, 2505, 25, 1207, 16, 25, 3555, 374, 220, 17, 10, 17, 30, 3070, 32, 8, 220, 19, 334, 425, 8, 220, 20, 198, 5097, 25, 5212, 7841, 788, 330, 3838, 374, 220, 17, 10, 17, 31011, 330, 24710, 788, 5212, 16, 788, 330, 19, 497, 330, 17, 788, 330, 20, 14345, 330, 19928, 788, 330, 16, 63159, 2505, 25, 18374, 315, 17689, 30, 264, 8, 24081, 293, 8, 26937, 272, 8, 60249, 13, 21806, 25, 264, 198, 5097, 25, 5212, 7841, 788, 330, 63593, 315, 17689, 31011, 330, 24710, 788, 5212, 16, 788, 330, 37036, 1869, 497, 330, 17, 788, 330, 3428, 22365, 497, 330, 18, 788, 330, 2208, 8792, 14345, 330, 19928, 788, 330, 16, 63159, 2505, 25, 15920, 374, 4396, 30, 3070, 64, 8, 1599, 334, 3070, 65, 8, 809, 334, 3070, 66, 8, 1863, 1019, 5097, 25, 5212, 7841, 788, 330, 23085, 374, 4396, 31011, 330, 24710, 788, 5212, 16, 788, 330, 55, 497, 330, 17, 788, 330, 56, 497, 330, 18, 788, 330, 57, 14345, 330, 19928, 788, 330, 15, 9207, 151645, 198, 151644, 872, 198, 12012, 279, 2701, 5248, 62626, 3405, 1119, 4718, 624, 2679, 432, 374, 537, 264, 2697, 3405, 11, 2550, 81404, 624, 21, 18, 13, 41115, 264, 1051, 396, 1187, 10946, 94859, 5267, 16, 8, 52763, 66183, 198, 17, 8, 34307, 5242, 479, 63936, 198, 18, 8, 65386, 2994, 96766, 198, 19, 8, 44406, 87910, 198, 334, 33092, 25, 220, 18, 334, 151645, 198, 151644, 77091, 198, 4913, 7841, 788, 330, 61445, 264, 1051, 396, 1187, 10946, 94859, 31011, 330, 24710, 788, 5212, 16, 788, 330, 47, 31460, 66183, 497, 330, 17, 788, 330, 71992, 1168, 5242, 479, 63936, 497, 330, 18, 788, 330, 71904, 20473, 2994, 96766, 497, 330, 19, 788, 330, 19560, 301, 87910, 14345, 330, 19928, 788, 330, 18, 9207, 151645, 198]\n","inputs:\n","<|im_start|>system\n","You are a quiz parser. Output JSON for questions, \"SKIP\" for non-questions.\n","\n","Format: {\"question\": \"text\", \"answers\": {\"1\": \"opt1\", \"2\": \"opt2\"}, \"correct\": \"1\"}\n","\n","Key Rules:\n","1. Clean: remove numbering (Q1:, 1., etc.) and formatting markers\n","2. Answers: always use keys \"1\", \"2\", \"3\"\n","3. Correct: find from bold/**text**, [highlight], ✅, [colored], or trailing indicators\n","4. CRITICAL: All options same format → \"0\"\n","5. CRITICAL: 2+ marked OR no marking → \"0\"\n","6. Output ONLY JSON or \"SKIP\"\n","\n","Examples:\n","Input: Q1: What is 2+2? **A) 4** B) 5\n","Output: {\"question\": \"What is 2+2?\", \"answers\": {\"1\": \"4\", \"2\": \"5\"}, \"correct\": \"1\"}\n","\n","Input: Capital of Spain? a) Madrid b) Barcelona c) Valencia. Answer: a\n","Output: {\"question\": \"Capital of Spain?\", \"answers\": {\"1\": \"Madrid\", \"2\": \"Barcelona\", \"3\": \"Valencia\"}, \"correct\": \"1\"}\n","\n","Input: Which is correct? **a) X** **b) Y** **c) Z**\n","Output: {\"question\": \"Which is correct?\", \"answers\": {\"1\": \"X\", \"2\": \"Y\", \"3\": \"Z\"}, \"correct\": \"0\"}<|im_end|>\n","<|im_start|>user\n","Convert the following multiple-choice question into JSON.\n","If it is not a valid question, output SKIP.\n","63. Qui a peint la Joconde?\n","1) Pablo Picasso\n","2) Vincent van Gogh\n","3) Leonardo da Vinci\n","4) Michelangelo\n","**Correct: 3**<|im_end|>\n","<|im_start|>assistant\n","{\"question\": \"Qui a peint la Joconde?\", \"answers\": {\"1\": \"Pablo Picasso\", \"2\": \"Vincent van Gogh\", \"3\": \"Leonardo da Vinci\", \"4\": \"Michelangelo\"}, \"correct\": \"3\"}<|im_end|>\n","\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 7841, 788, 330, 61445, 264, 1051, 396, 1187, 10946, 94859, 31011, 330, 24710, 788, 5212, 16, 788, 330, 47, 31460, 66183, 497, 330, 17, 788, 330, 71992, 1168, 5242, 479, 63936, 497, 330, 18, 788, 330, 71904, 20473, 2994, 96766, 497, 330, 19, 788, 330, 19560, 301, 87910, 14345, 330, 19928, 788, 330, 18, 9207, 151645, 198]\n","labels:\n","{\"question\": \"Qui a peint la Joconde?\", \"answers\": {\"1\": \"Pablo Picasso\", \"2\": \"Vincent van Gogh\", \"3\": \"Leonardo da Vinci\", \"4\": \"Michelangelo\"}, \"correct\": \"3\"}<|im_end|>\n","\n","[INFO|configuration_utils.py:765] 2025-12-22 11:43:06,376 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 11:43:06,377 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|2025-12-22 11:43:06] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n","[WARNING|logging.py:328] 2025-12-22 11:43:06,970 >> `torch_dtype` is deprecated! Use `dtype` instead!\n","model.safetensors: 100% 3.09G/3.09G [00:51<00:00, 60.0MB/s]\n","[INFO|modeling_utils.py:1172] 2025-12-22 11:43:58,681 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors\n","[INFO|modeling_utils.py:2341] 2025-12-22 11:43:58,684 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:986] 2025-12-22 11:43:58,687 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"use_cache\": false\n","}\n","\n","generation_config.json: 100% 242/242 [00:00<00:00, 1.42MB/s]\n","[INFO|configuration_utils.py:941] 2025-12-22 11:44:10,523 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json\n","[INFO|configuration_utils.py:986] 2025-12-22 11:44:10,523 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    151645,\n","    151643\n","  ],\n","  \"pad_token_id\": 151643,\n","  \"repetition_penalty\": 1.1,\n","  \"temperature\": 0.7,\n","  \"top_k\": 20,\n","  \"top_p\": 0.8\n","}\n","\n","[INFO|dynamic_module_utils.py:423] 2025-12-22 11:44:10,610 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-1.5B-Instruct.\n","[INFO|2025-12-22 11:44:10] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n","[INFO|2025-12-22 11:44:10] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-12-22 11:44:10] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n","[INFO|2025-12-22 11:44:10] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n","[INFO|2025-12-22 11:44:10] llamafactory.model.model_utils.misc:143 >> Found linear modules: v_proj,q_proj,k_proj,up_proj,gate_proj,o_proj,down_proj\n","[INFO|2025-12-22 11:44:19] llamafactory.model.loader:143 >> trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945\n","[WARNING|trainer.py:906] 2025-12-22 11:44:19,084 >> The model is already on multiple devices. Skipping the move to device specified in `args`.\n","[INFO|trainer.py:749] 2025-12-22 11:44:19,198 >> Using auto half precision backend\n","[WARNING|trainer.py:982] 2025-12-22 11:44:19,204 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","[INFO|trainer.py:1335] 2025-12-22 11:44:22,633 >> skipped Embedding(151936, 1536): 222.5625M params\n","[INFO|trainer.py:1338] 2025-12-22 11:44:22,650 >> skipped: 222.5625M params\n","[INFO|trainer.py:2519] 2025-12-22 11:44:23,035 >> ***** Running training *****\n","[INFO|trainer.py:2520] 2025-12-22 11:44:23,035 >>   Num examples = 2,197\n","[INFO|trainer.py:2521] 2025-12-22 11:44:23,035 >>   Num Epochs = 3\n","[INFO|trainer.py:2522] 2025-12-22 11:44:23,035 >>   Instantaneous batch size per device = 1\n","[INFO|trainer.py:2525] 2025-12-22 11:44:23,035 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2526] 2025-12-22 11:44:23,035 >>   Gradient Accumulation steps = 16\n","[INFO|trainer.py:2527] 2025-12-22 11:44:23,035 >>   Total optimization steps = 414\n","[INFO|trainer.py:2528] 2025-12-22 11:44:23,077 >>   Number of trainable parameters = 9,232,384\n","{'loss': 0.0771, 'grad_norm': 0.244588702917099, 'learning_rate': 2.1428571428571428e-05, 'epoch': 0.07}\n","{'loss': 0.0544, 'grad_norm': 0.2609480023384094, 'learning_rate': 4.523809523809524e-05, 'epoch': 0.15}\n","{'loss': 0.0479, 'grad_norm': 0.19627851247787476, 'learning_rate': 6.904761904761905e-05, 'epoch': 0.22}\n","{'loss': 0.0293, 'grad_norm': 0.1777338832616806, 'learning_rate': 9.285714285714286e-05, 'epoch': 0.29}\n","{'loss': 0.0206, 'grad_norm': 0.21973486244678497, 'learning_rate': 9.991265793145479e-05, 'epoch': 0.36}\n","{'loss': 0.0167, 'grad_norm': 0.2934594750404358, 'learning_rate': 9.948559446825412e-05, 'epoch': 0.44}\n","{'loss': 0.015, 'grad_norm': 0.21600155532360077, 'learning_rate': 9.870580739976935e-05, 'epoch': 0.51}\n","{'loss': 0.0122, 'grad_norm': 0.09629154205322266, 'learning_rate': 9.757885489518297e-05, 'epoch': 0.58}\n","{'loss': 0.0082, 'grad_norm': 0.25241777300834656, 'learning_rate': 9.611276965077099e-05, 'epoch': 0.66}\n","{'loss': 0.0101, 'grad_norm': 0.04962103068828583, 'learning_rate': 9.431800163442041e-05, 'epoch': 0.73}\n"," 24% 100/414 [12:33<38:40,  7.39s/it][INFO|trainer.py:4643] 2025-12-22 11:56:56,565 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4645] 2025-12-22 11:56:56,565 >>   Num examples = 251\n","[INFO|trainer.py:4648] 2025-12-22 11:56:56,565 >>   Batch size = 1\n","\n","  0% 0/251 [00:00<?, ?it/s]\u001b[A\n","  1% 2/251 [00:00<00:25,  9.79it/s]\u001b[A\n","  1% 3/251 [00:00<00:27,  9.03it/s]\u001b[A\n","  2% 4/251 [00:00<00:29,  8.47it/s]\u001b[A\n","  2% 5/251 [00:00<00:32,  7.57it/s]\u001b[A\n","  2% 6/251 [00:00<00:32,  7.51it/s]\u001b[A\n","  3% 7/251 [00:00<00:30,  7.88it/s]\u001b[A\n","  3% 8/251 [00:00<00:31,  7.78it/s]\u001b[A\n","  4% 9/251 [00:01<00:33,  7.19it/s]\u001b[A\n","  4% 10/251 [00:01<00:33,  7.12it/s]\u001b[A\n","  4% 11/251 [00:01<00:36,  6.52it/s]\u001b[A\n","  5% 12/251 [00:01<00:35,  6.79it/s]\u001b[A\n","  5% 13/251 [00:01<00:34,  6.97it/s]\u001b[A\n","  6% 14/251 [00:01<00:35,  6.69it/s]\u001b[A\n","  6% 15/251 [00:02<00:34,  6.81it/s]\u001b[A\n","  6% 16/251 [00:02<00:33,  7.04it/s]\u001b[A\n","  7% 17/251 [00:02<00:32,  7.12it/s]\u001b[A\n","  7% 18/251 [00:02<00:32,  7.25it/s]\u001b[A\n","  8% 19/251 [00:02<00:31,  7.36it/s]\u001b[A\n","  8% 20/251 [00:02<00:33,  6.94it/s]\u001b[A\n","  8% 21/251 [00:02<00:32,  7.17it/s]\u001b[A\n","  9% 22/251 [00:03<00:31,  7.33it/s]\u001b[A\n","  9% 23/251 [00:03<00:32,  6.99it/s]\u001b[A\n"," 10% 24/251 [00:03<00:34,  6.56it/s]\u001b[A\n"," 10% 25/251 [00:03<00:33,  6.79it/s]\u001b[A\n"," 10% 26/251 [00:03<00:35,  6.42it/s]\u001b[A\n"," 11% 27/251 [00:03<00:33,  6.64it/s]\u001b[A\n"," 11% 28/251 [00:03<00:32,  6.89it/s]\u001b[A\n"," 12% 29/251 [00:04<00:33,  6.72it/s]\u001b[A\n"," 12% 30/251 [00:04<00:33,  6.62it/s]\u001b[A\n"," 12% 31/251 [00:04<00:39,  5.57it/s]\u001b[A\n"," 13% 32/251 [00:04<00:36,  5.95it/s]\u001b[A\n"," 13% 33/251 [00:04<00:34,  6.28it/s]\u001b[A\n"," 14% 34/251 [00:04<00:32,  6.58it/s]\u001b[A\n"," 14% 35/251 [00:05<00:31,  6.79it/s]\u001b[A\n"," 14% 36/251 [00:05<00:32,  6.55it/s]\u001b[A\n"," 15% 37/251 [00:05<00:33,  6.37it/s]\u001b[A\n"," 15% 38/251 [00:05<00:32,  6.46it/s]\u001b[A\n"," 16% 39/251 [00:05<00:32,  6.47it/s]\u001b[A\n"," 16% 40/251 [00:05<00:31,  6.62it/s]\u001b[A\n"," 16% 41/251 [00:05<00:31,  6.62it/s]\u001b[A\n"," 17% 42/251 [00:06<00:32,  6.53it/s]\u001b[A\n"," 17% 43/251 [00:06<00:32,  6.41it/s]\u001b[A\n"," 18% 44/251 [00:06<00:32,  6.46it/s]\u001b[A\n"," 18% 45/251 [00:06<00:30,  6.82it/s]\u001b[A\n"," 18% 46/251 [00:06<00:28,  7.20it/s]\u001b[A\n"," 19% 47/251 [00:06<00:27,  7.44it/s]\u001b[A\n"," 19% 48/251 [00:06<00:27,  7.29it/s]\u001b[A\n"," 20% 49/251 [00:07<00:27,  7.42it/s]\u001b[A\n"," 20% 50/251 [00:07<00:26,  7.51it/s]\u001b[A\n"," 20% 51/251 [00:07<00:28,  7.03it/s]\u001b[A\n"," 21% 52/251 [00:07<00:28,  6.87it/s]\u001b[A\n"," 21% 53/251 [00:07<00:29,  6.70it/s]\u001b[A\n"," 22% 54/251 [00:07<00:28,  6.86it/s]\u001b[A\n"," 22% 55/251 [00:07<00:27,  7.03it/s]\u001b[A\n"," 22% 56/251 [00:08<00:27,  7.09it/s]\u001b[A\n"," 23% 57/251 [00:08<00:26,  7.28it/s]\u001b[A\n"," 23% 58/251 [00:08<00:26,  7.33it/s]\u001b[A\n"," 24% 59/251 [00:08<00:26,  7.17it/s]\u001b[A\n"," 24% 60/251 [00:08<00:25,  7.37it/s]\u001b[A\n"," 24% 61/251 [00:08<00:24,  7.61it/s]\u001b[A\n"," 25% 62/251 [00:08<00:24,  7.61it/s]\u001b[A\n"," 25% 63/251 [00:09<00:24,  7.66it/s]\u001b[A\n"," 25% 64/251 [00:09<00:26,  7.09it/s]\u001b[A\n"," 26% 65/251 [00:09<00:25,  7.31it/s]\u001b[A\n"," 26% 66/251 [00:09<00:25,  7.13it/s]\u001b[A\n"," 27% 67/251 [00:09<00:28,  6.48it/s]\u001b[A\n"," 27% 68/251 [00:09<00:28,  6.43it/s]\u001b[A\n"," 27% 69/251 [00:09<00:27,  6.64it/s]\u001b[A\n"," 28% 70/251 [00:10<00:26,  6.91it/s]\u001b[A\n"," 28% 71/251 [00:10<00:25,  7.12it/s]\u001b[A\n"," 29% 72/251 [00:10<00:24,  7.33it/s]\u001b[A\n"," 29% 73/251 [00:10<00:23,  7.44it/s]\u001b[A\n"," 29% 74/251 [00:10<00:27,  6.52it/s]\u001b[A\n"," 30% 75/251 [00:10<00:26,  6.76it/s]\u001b[A\n"," 30% 76/251 [00:10<00:25,  6.83it/s]\u001b[A\n"," 31% 77/251 [00:11<00:25,  6.92it/s]\u001b[A\n"," 31% 78/251 [00:11<00:24,  7.06it/s]\u001b[A\n"," 31% 79/251 [00:11<00:24,  6.96it/s]\u001b[A\n"," 32% 80/251 [00:11<00:23,  7.21it/s]\u001b[A\n"," 32% 81/251 [00:11<00:23,  7.39it/s]\u001b[A\n"," 33% 82/251 [00:11<00:24,  6.84it/s]\u001b[A\n"," 33% 83/251 [00:11<00:25,  6.70it/s]\u001b[A\n"," 33% 84/251 [00:12<00:24,  6.79it/s]\u001b[A\n"," 34% 85/251 [00:12<00:23,  7.15it/s]\u001b[A\n"," 34% 86/251 [00:12<00:24,  6.87it/s]\u001b[A\n"," 35% 87/251 [00:12<00:22,  7.44it/s]\u001b[A\n"," 35% 88/251 [00:12<00:21,  7.42it/s]\u001b[A\n"," 35% 89/251 [00:12<00:21,  7.46it/s]\u001b[A\n"," 36% 90/251 [00:12<00:22,  7.01it/s]\u001b[A\n"," 36% 91/251 [00:13<00:22,  7.16it/s]\u001b[A\n"," 37% 92/251 [00:13<00:21,  7.29it/s]\u001b[A\n"," 37% 93/251 [00:13<00:21,  7.25it/s]\u001b[A\n"," 37% 94/251 [00:13<00:21,  7.19it/s]\u001b[A\n"," 38% 95/251 [00:13<00:22,  7.02it/s]\u001b[A\n"," 38% 96/251 [00:13<00:23,  6.71it/s]\u001b[A\n"," 39% 97/251 [00:13<00:23,  6.52it/s]\u001b[A\n"," 39% 98/251 [00:14<00:22,  6.79it/s]\u001b[A\n"," 39% 99/251 [00:14<00:22,  6.63it/s]\u001b[A\n"," 40% 100/251 [00:14<00:21,  6.88it/s]\u001b[A\n"," 40% 101/251 [00:14<00:20,  7.25it/s]\u001b[A\n"," 41% 102/251 [00:14<00:21,  6.99it/s]\u001b[A\n"," 41% 103/251 [00:14<00:20,  7.05it/s]\u001b[A\n"," 41% 104/251 [00:14<00:21,  6.91it/s]\u001b[A\n"," 42% 105/251 [00:15<00:21,  6.72it/s]\u001b[A\n"," 42% 106/251 [00:15<00:22,  6.57it/s]\u001b[A\n"," 43% 107/251 [00:15<00:21,  6.79it/s]\u001b[A\n"," 43% 108/251 [00:15<00:20,  6.99it/s]\u001b[A\n"," 43% 109/251 [00:15<00:19,  7.21it/s]\u001b[A\n"," 44% 110/251 [00:15<00:20,  7.03it/s]\u001b[A\n"," 44% 111/251 [00:15<00:20,  6.73it/s]\u001b[A\n"," 45% 112/251 [00:16<00:19,  6.97it/s]\u001b[A\n"," 45% 113/251 [00:16<00:19,  7.21it/s]\u001b[A\n"," 45% 114/251 [00:16<00:18,  7.35it/s]\u001b[A\n"," 46% 115/251 [00:16<00:19,  7.13it/s]\u001b[A\n"," 46% 116/251 [00:16<00:18,  7.18it/s]\u001b[A\n"," 47% 117/251 [00:16<00:18,  7.23it/s]\u001b[A\n"," 47% 118/251 [00:16<00:19,  6.93it/s]\u001b[A\n"," 47% 119/251 [00:17<00:19,  6.81it/s]\u001b[A\n"," 48% 120/251 [00:17<00:18,  6.90it/s]\u001b[A\n"," 48% 121/251 [00:17<00:18,  6.88it/s]\u001b[A\n"," 49% 122/251 [00:17<00:19,  6.66it/s]\u001b[A\n"," 49% 123/251 [00:17<00:18,  6.75it/s]\u001b[A\n"," 49% 124/251 [00:17<00:18,  6.99it/s]\u001b[A\n"," 50% 125/251 [00:17<00:18,  6.78it/s]\u001b[A\n"," 50% 126/251 [00:18<00:18,  6.90it/s]\u001b[A\n"," 51% 127/251 [00:18<00:17,  7.02it/s]\u001b[A\n"," 51% 128/251 [00:18<00:18,  6.79it/s]\u001b[A\n"," 51% 129/251 [00:18<00:18,  6.65it/s]\u001b[A\n"," 52% 130/251 [00:18<00:17,  6.84it/s]\u001b[A\n"," 52% 131/251 [00:18<00:17,  6.72it/s]\u001b[A\n"," 53% 132/251 [00:18<00:17,  6.81it/s]\u001b[A\n"," 53% 133/251 [00:19<00:18,  6.31it/s]\u001b[A\n"," 53% 134/251 [00:19<00:18,  6.33it/s]\u001b[A\n"," 54% 135/251 [00:19<00:17,  6.56it/s]\u001b[A\n"," 54% 136/251 [00:19<00:16,  6.84it/s]\u001b[A\n"," 55% 137/251 [00:19<00:16,  6.71it/s]\u001b[A\n"," 55% 138/251 [00:19<00:16,  6.93it/s]\u001b[A\n"," 55% 139/251 [00:20<00:16,  6.81it/s]\u001b[A\n"," 56% 140/251 [00:20<00:15,  7.06it/s]\u001b[A\n"," 56% 141/251 [00:20<00:15,  6.97it/s]\u001b[A\n"," 57% 142/251 [00:20<00:15,  7.17it/s]\u001b[A\n"," 57% 143/251 [00:20<00:14,  7.31it/s]\u001b[A\n"," 57% 144/251 [00:20<00:15,  6.86it/s]\u001b[A\n"," 58% 145/251 [00:20<00:15,  7.01it/s]\u001b[A\n"," 58% 146/251 [00:21<00:14,  7.19it/s]\u001b[A\n"," 59% 147/251 [00:21<00:14,  7.34it/s]\u001b[A\n"," 59% 148/251 [00:21<00:13,  7.48it/s]\u001b[A\n"," 59% 149/251 [00:21<00:13,  7.41it/s]\u001b[A\n"," 60% 150/251 [00:21<00:13,  7.47it/s]\u001b[A\n"," 60% 151/251 [00:21<00:14,  7.01it/s]\u001b[A\n"," 61% 152/251 [00:21<00:14,  6.64it/s]\u001b[A\n"," 61% 153/251 [00:22<00:16,  6.11it/s]\u001b[A\n"," 61% 154/251 [00:22<00:15,  6.45it/s]\u001b[A\n"," 62% 155/251 [00:22<00:14,  6.66it/s]\u001b[A\n"," 62% 156/251 [00:22<00:13,  6.94it/s]\u001b[A\n"," 63% 157/251 [00:22<00:13,  7.14it/s]\u001b[A\n"," 63% 158/251 [00:22<00:14,  6.27it/s]\u001b[A\n"," 63% 159/251 [00:22<00:13,  6.60it/s]\u001b[A\n"," 64% 160/251 [00:23<00:12,  7.26it/s]\u001b[A\n"," 64% 161/251 [00:23<00:12,  7.20it/s]\u001b[A\n"," 65% 162/251 [00:23<00:12,  7.22it/s]\u001b[A\n"," 65% 163/251 [00:23<00:12,  6.95it/s]\u001b[A\n"," 65% 164/251 [00:23<00:12,  7.04it/s]\u001b[A\n"," 66% 165/251 [00:23<00:12,  7.10it/s]\u001b[A\n"," 66% 166/251 [00:23<00:11,  7.28it/s]\u001b[A\n"," 67% 167/251 [00:23<00:10,  7.68it/s]\u001b[A\n"," 67% 168/251 [00:24<00:10,  7.77it/s]\u001b[A\n"," 67% 169/251 [00:24<00:10,  7.70it/s]\u001b[A\n"," 68% 170/251 [00:24<00:10,  7.77it/s]\u001b[A\n"," 68% 171/251 [00:24<00:11,  6.77it/s]\u001b[A\n"," 69% 172/251 [00:24<00:12,  6.44it/s]\u001b[A\n"," 69% 173/251 [00:24<00:11,  6.78it/s]\u001b[A\n"," 69% 174/251 [00:25<00:12,  6.27it/s]\u001b[A\n"," 70% 175/251 [00:25<00:11,  6.58it/s]\u001b[A\n"," 70% 176/251 [00:25<00:11,  6.78it/s]\u001b[A\n"," 71% 177/251 [00:25<00:10,  7.20it/s]\u001b[A\n"," 71% 178/251 [00:25<00:10,  7.25it/s]\u001b[A\n"," 71% 179/251 [00:25<00:09,  7.35it/s]\u001b[A\n"," 72% 180/251 [00:25<00:09,  7.46it/s]\u001b[A\n"," 72% 181/251 [00:25<00:09,  7.55it/s]\u001b[A\n"," 73% 182/251 [00:26<00:09,  7.47it/s]\u001b[A\n"," 73% 183/251 [00:26<00:09,  7.35it/s]\u001b[A\n"," 73% 184/251 [00:26<00:09,  7.42it/s]\u001b[A\n"," 74% 185/251 [00:26<00:08,  7.38it/s]\u001b[A\n"," 74% 186/251 [00:26<00:08,  7.58it/s]\u001b[A\n"," 75% 187/251 [00:26<00:08,  7.32it/s]\u001b[A\n"," 75% 188/251 [00:26<00:09,  6.53it/s]\u001b[A\n"," 75% 189/251 [00:27<00:09,  6.77it/s]\u001b[A\n"," 76% 190/251 [00:27<00:08,  7.01it/s]\u001b[A\n"," 76% 191/251 [00:27<00:08,  6.79it/s]\u001b[A\n"," 76% 192/251 [00:27<00:08,  6.98it/s]\u001b[A\n"," 77% 193/251 [00:27<00:08,  7.14it/s]\u001b[A\n"," 77% 194/251 [00:27<00:07,  7.26it/s]\u001b[A\n"," 78% 195/251 [00:27<00:08,  6.82it/s]\u001b[A\n"," 78% 196/251 [00:28<00:07,  7.06it/s]\u001b[A\n"," 78% 197/251 [00:28<00:07,  6.79it/s]\u001b[A\n"," 79% 198/251 [00:28<00:07,  6.98it/s]\u001b[A\n"," 79% 199/251 [00:28<00:07,  7.17it/s]\u001b[A\n"," 80% 200/251 [00:28<00:07,  7.22it/s]\u001b[A\n"," 80% 201/251 [00:28<00:07,  6.72it/s]\u001b[A\n"," 80% 202/251 [00:28<00:07,  6.81it/s]\u001b[A\n"," 81% 203/251 [00:29<00:06,  7.10it/s]\u001b[A\n"," 81% 204/251 [00:29<00:06,  7.30it/s]\u001b[A\n"," 82% 205/251 [00:29<00:06,  6.98it/s]\u001b[A\n"," 82% 206/251 [00:29<00:06,  6.61it/s]\u001b[A\n"," 82% 207/251 [00:29<00:06,  6.45it/s]\u001b[A\n"," 83% 208/251 [00:29<00:06,  6.74it/s]\u001b[A\n"," 83% 209/251 [00:29<00:06,  6.88it/s]\u001b[A\n"," 84% 210/251 [00:30<00:05,  7.00it/s]\u001b[A\n"," 84% 211/251 [00:30<00:05,  7.18it/s]\u001b[A\n"," 84% 212/251 [00:30<00:05,  7.09it/s]\u001b[A\n"," 85% 213/251 [00:30<00:05,  6.94it/s]\u001b[A\n"," 85% 214/251 [00:30<00:05,  6.98it/s]\u001b[A\n"," 86% 215/251 [00:30<00:05,  7.14it/s]\u001b[A\n"," 86% 216/251 [00:31<00:05,  6.70it/s]\u001b[A\n"," 86% 217/251 [00:31<00:04,  6.86it/s]\u001b[A\n"," 87% 218/251 [00:31<00:04,  7.09it/s]\u001b[A\n"," 87% 219/251 [00:31<00:04,  7.08it/s]\u001b[A\n"," 88% 220/251 [00:31<00:04,  7.05it/s]\u001b[A\n"," 88% 221/251 [00:31<00:04,  7.11it/s]\u001b[A\n"," 88% 222/251 [00:31<00:03,  7.31it/s]\u001b[A\n"," 89% 223/251 [00:31<00:04,  6.78it/s]\u001b[A\n"," 89% 224/251 [00:32<00:04,  6.59it/s]\u001b[A\n"," 90% 225/251 [00:32<00:04,  5.62it/s]\u001b[A\n"," 90% 226/251 [00:32<00:04,  5.67it/s]\u001b[A\n"," 90% 227/251 [00:32<00:03,  6.15it/s]\u001b[A\n"," 91% 228/251 [00:32<00:03,  6.52it/s]\u001b[A\n"," 91% 229/251 [00:32<00:03,  6.84it/s]\u001b[A\n"," 92% 230/251 [00:33<00:03,  6.94it/s]\u001b[A\n"," 92% 231/251 [00:33<00:02,  7.08it/s]\u001b[A\n"," 92% 232/251 [00:33<00:02,  7.18it/s]\u001b[A\n"," 93% 233/251 [00:33<00:02,  7.13it/s]\u001b[A\n"," 93% 234/251 [00:33<00:02,  7.25it/s]\u001b[A\n"," 94% 235/251 [00:33<00:02,  7.32it/s]\u001b[A\n"," 94% 236/251 [00:33<00:02,  7.28it/s]\u001b[A\n"," 94% 237/251 [00:34<00:01,  7.45it/s]\u001b[A\n"," 95% 238/251 [00:34<00:01,  7.52it/s]\u001b[A\n"," 95% 239/251 [00:34<00:01,  7.60it/s]\u001b[A\n"," 96% 240/251 [00:34<00:01,  7.68it/s]\u001b[A\n"," 96% 241/251 [00:34<00:01,  7.63it/s]\u001b[A\n"," 96% 242/251 [00:34<00:01,  7.62it/s]\u001b[A\n"," 97% 243/251 [00:34<00:01,  7.56it/s]\u001b[A\n"," 97% 244/251 [00:34<00:00,  7.54it/s]\u001b[A\n"," 98% 245/251 [00:35<00:00,  7.58it/s]\u001b[A\n"," 98% 246/251 [00:35<00:00,  7.66it/s]\u001b[A\n"," 98% 247/251 [00:35<00:00,  7.94it/s]\u001b[A\n"," 99% 248/251 [00:35<00:00,  7.92it/s]\u001b[A\n"," 99% 249/251 [00:35<00:00,  7.77it/s]\u001b[A\n","100% 250/251 [00:35<00:00,  7.79it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.012818524613976479, 'eval_runtime': 36.1681, 'eval_samples_per_second': 6.94, 'eval_steps_per_second': 6.94, 'epoch': 0.73}\n"," 24% 100/414 [13:09<38:40,  7.39s/it]\n","100% 251/251 [00:35<00:00,  7.13it/s]\u001b[A\n","{'loss': 0.01, 'grad_norm': 0.12945453822612762, 'learning_rate': 9.220734360030907e-05, 'epoch': 0.8}\n","{'loss': 0.0152, 'grad_norm': 0.1373288333415985, 'learning_rate': 8.979583990466454e-05, 'epoch': 0.87}\n","{'loss': 0.0152, 'grad_norm': 0.12635581195354462, 'learning_rate': 8.710067927254555e-05, 'epoch': 0.95}\n","{'loss': 0.0095, 'grad_norm': 0.12081838399171829, 'learning_rate': 8.414107227998329e-05, 'epoch': 1.01}\n","{'loss': 0.0048, 'grad_norm': 0.15049241483211517, 'learning_rate': 8.093811442476573e-05, 'epoch': 1.09}\n","{'loss': 0.0058, 'grad_norm': 0.06474736332893372, 'learning_rate': 7.751463576186957e-05, 'epoch': 1.16}\n","{'loss': 0.0066, 'grad_norm': 0.10269124060869217, 'learning_rate': 7.389503817530905e-05, 'epoch': 1.23}\n","{'loss': 0.0047, 'grad_norm': 0.031423091888427734, 'learning_rate': 7.010512144629579e-05, 'epoch': 1.31}\n","{'loss': 0.0062, 'grad_norm': 0.08375244587659836, 'learning_rate': 6.61718993574619e-05, 'epoch': 1.38}\n","{'loss': 0.0032, 'grad_norm': 0.04555759206414223, 'learning_rate': 6.2123407143919e-05, 'epoch': 1.45}\n"," 48% 200/414 [25:31<26:53,  7.54s/it][INFO|trainer.py:4643] 2025-12-22 12:09:54,991 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4645] 2025-12-22 12:09:54,992 >>   Num examples = 251\n","[INFO|trainer.py:4648] 2025-12-22 12:09:54,992 >>   Batch size = 1\n","\n","  0% 0/251 [00:00<?, ?it/s]\u001b[A\n","  1% 2/251 [00:00<00:25,  9.74it/s]\u001b[A\n","  1% 3/251 [00:00<00:27,  8.86it/s]\u001b[A\n","  2% 4/251 [00:00<00:29,  8.32it/s]\u001b[A\n","  2% 5/251 [00:00<00:33,  7.45it/s]\u001b[A\n","  2% 6/251 [00:00<00:32,  7.43it/s]\u001b[A\n","  3% 7/251 [00:00<00:31,  7.83it/s]\u001b[A\n","  3% 8/251 [00:01<00:31,  7.80it/s]\u001b[A\n","  4% 9/251 [00:01<00:33,  7.17it/s]\u001b[A\n","  4% 10/251 [00:01<00:33,  7.15it/s]\u001b[A\n","  4% 11/251 [00:01<00:36,  6.52it/s]\u001b[A\n","  5% 12/251 [00:01<00:35,  6.73it/s]\u001b[A\n","  5% 13/251 [00:01<00:34,  6.92it/s]\u001b[A\n","  6% 14/251 [00:01<00:35,  6.68it/s]\u001b[A\n","  6% 15/251 [00:02<00:34,  6.82it/s]\u001b[A\n","  6% 16/251 [00:02<00:33,  7.08it/s]\u001b[A\n","  7% 17/251 [00:02<00:32,  7.16it/s]\u001b[A\n","  7% 18/251 [00:02<00:32,  7.27it/s]\u001b[A\n","  8% 19/251 [00:02<00:31,  7.37it/s]\u001b[A\n","  8% 20/251 [00:02<00:33,  6.94it/s]\u001b[A\n","  8% 21/251 [00:02<00:32,  7.16it/s]\u001b[A\n","  9% 22/251 [00:03<00:31,  7.29it/s]\u001b[A\n","  9% 23/251 [00:03<00:32,  6.98it/s]\u001b[A\n"," 10% 24/251 [00:03<00:33,  6.76it/s]\u001b[A\n"," 10% 25/251 [00:03<00:32,  6.97it/s]\u001b[A\n"," 10% 26/251 [00:03<00:34,  6.60it/s]\u001b[A\n"," 11% 27/251 [00:03<00:32,  6.80it/s]\u001b[A\n"," 11% 28/251 [00:03<00:31,  7.00it/s]\u001b[A\n"," 12% 29/251 [00:04<00:32,  6.79it/s]\u001b[A\n"," 12% 30/251 [00:04<00:31,  6.96it/s]\u001b[A\n"," 12% 31/251 [00:04<00:38,  5.75it/s]\u001b[A\n"," 13% 32/251 [00:04<00:35,  6.19it/s]\u001b[A\n"," 13% 33/251 [00:04<00:33,  6.48it/s]\u001b[A\n"," 14% 34/251 [00:04<00:32,  6.77it/s]\u001b[A\n"," 14% 35/251 [00:04<00:30,  7.00it/s]\u001b[A\n"," 14% 36/251 [00:05<00:31,  6.75it/s]\u001b[A\n"," 15% 37/251 [00:05<00:32,  6.55it/s]\u001b[A\n"," 15% 38/251 [00:05<00:31,  6.74it/s]\u001b[A\n"," 16% 39/251 [00:05<00:30,  6.99it/s]\u001b[A\n"," 16% 40/251 [00:05<00:30,  6.98it/s]\u001b[A\n"," 16% 41/251 [00:05<00:29,  7.13it/s]\u001b[A\n"," 17% 42/251 [00:05<00:29,  7.10it/s]\u001b[A\n"," 17% 43/251 [00:06<00:30,  6.84it/s]\u001b[A\n"," 18% 44/251 [00:06<00:30,  6.72it/s]\u001b[A\n"," 18% 45/251 [00:06<00:29,  7.04it/s]\u001b[A\n"," 18% 46/251 [00:06<00:28,  7.32it/s]\u001b[A\n"," 19% 47/251 [00:06<00:27,  7.53it/s]\u001b[A\n"," 19% 48/251 [00:06<00:28,  7.25it/s]\u001b[A\n"," 20% 49/251 [00:06<00:27,  7.34it/s]\u001b[A\n"," 20% 50/251 [00:07<00:26,  7.48it/s]\u001b[A\n"," 20% 51/251 [00:07<00:28,  6.99it/s]\u001b[A\n"," 21% 52/251 [00:07<00:29,  6.85it/s]\u001b[A\n"," 21% 53/251 [00:07<00:29,  6.69it/s]\u001b[A\n"," 22% 54/251 [00:07<00:28,  6.83it/s]\u001b[A\n"," 22% 55/251 [00:07<00:28,  6.94it/s]\u001b[A\n"," 22% 56/251 [00:07<00:27,  7.05it/s]\u001b[A\n"," 23% 57/251 [00:08<00:26,  7.29it/s]\u001b[A\n"," 23% 58/251 [00:08<00:26,  7.29it/s]\u001b[A\n"," 24% 59/251 [00:08<00:27,  7.10it/s]\u001b[A\n"," 24% 60/251 [00:08<00:26,  7.30it/s]\u001b[A\n"," 24% 61/251 [00:08<00:26,  7.19it/s]\u001b[A\n"," 25% 62/251 [00:08<00:26,  7.14it/s]\u001b[A\n"," 25% 63/251 [00:08<00:26,  7.10it/s]\u001b[A\n"," 25% 64/251 [00:09<00:27,  6.76it/s]\u001b[A\n"," 26% 65/251 [00:09<00:26,  7.01it/s]\u001b[A\n"," 26% 66/251 [00:09<00:26,  6.92it/s]\u001b[A\n"," 27% 67/251 [00:09<00:29,  6.31it/s]\u001b[A\n"," 27% 68/251 [00:09<00:29,  6.27it/s]\u001b[A\n"," 27% 69/251 [00:09<00:27,  6.51it/s]\u001b[A\n"," 28% 70/251 [00:10<00:27,  6.65it/s]\u001b[A\n"," 28% 71/251 [00:10<00:25,  6.95it/s]\u001b[A\n"," 29% 72/251 [00:10<00:25,  7.09it/s]\u001b[A\n"," 29% 73/251 [00:10<00:24,  7.14it/s]\u001b[A\n"," 29% 74/251 [00:10<00:27,  6.39it/s]\u001b[A\n"," 30% 75/251 [00:10<00:26,  6.60it/s]\u001b[A\n"," 30% 76/251 [00:10<00:26,  6.69it/s]\u001b[A\n"," 31% 77/251 [00:11<00:26,  6.50it/s]\u001b[A\n"," 31% 78/251 [00:11<00:26,  6.62it/s]\u001b[A\n"," 31% 79/251 [00:11<00:26,  6.54it/s]\u001b[A\n"," 32% 80/251 [00:11<00:26,  6.45it/s]\u001b[A\n"," 32% 81/251 [00:11<00:26,  6.51it/s]\u001b[A\n"," 33% 82/251 [00:11<00:26,  6.29it/s]\u001b[A\n"," 33% 83/251 [00:12<00:26,  6.32it/s]\u001b[A\n"," 33% 84/251 [00:12<00:25,  6.52it/s]\u001b[A\n"," 34% 85/251 [00:12<00:23,  6.92it/s]\u001b[A\n"," 34% 86/251 [00:12<00:24,  6.63it/s]\u001b[A\n"," 35% 87/251 [00:12<00:22,  7.22it/s]\u001b[A\n"," 35% 88/251 [00:12<00:22,  7.32it/s]\u001b[A\n"," 35% 89/251 [00:12<00:21,  7.39it/s]\u001b[A\n"," 36% 90/251 [00:12<00:23,  6.98it/s]\u001b[A\n"," 36% 91/251 [00:13<00:22,  7.15it/s]\u001b[A\n"," 37% 92/251 [00:13<00:21,  7.28it/s]\u001b[A\n"," 37% 93/251 [00:13<00:21,  7.33it/s]\u001b[A\n"," 37% 94/251 [00:13<00:21,  7.17it/s]\u001b[A\n"," 38% 95/251 [00:13<00:22,  7.04it/s]\u001b[A\n"," 38% 96/251 [00:13<00:22,  6.77it/s]\u001b[A\n"," 39% 97/251 [00:13<00:23,  6.60it/s]\u001b[A\n"," 39% 98/251 [00:14<00:22,  6.85it/s]\u001b[A\n"," 39% 99/251 [00:14<00:22,  6.64it/s]\u001b[A\n"," 40% 100/251 [00:14<00:22,  6.84it/s]\u001b[A\n"," 40% 101/251 [00:14<00:20,  7.20it/s]\u001b[A\n"," 41% 102/251 [00:14<00:21,  6.94it/s]\u001b[A\n"," 41% 103/251 [00:14<00:20,  7.07it/s]\u001b[A\n"," 41% 104/251 [00:14<00:21,  6.89it/s]\u001b[A\n"," 42% 105/251 [00:15<00:21,  6.70it/s]\u001b[A\n"," 42% 106/251 [00:15<00:22,  6.55it/s]\u001b[A\n"," 43% 107/251 [00:15<00:21,  6.78it/s]\u001b[A\n"," 43% 108/251 [00:15<00:20,  6.99it/s]\u001b[A\n"," 43% 109/251 [00:15<00:19,  7.22it/s]\u001b[A\n"," 44% 110/251 [00:15<00:20,  7.03it/s]\u001b[A\n"," 44% 111/251 [00:16<00:20,  6.73it/s]\u001b[A\n"," 45% 112/251 [00:16<00:19,  7.00it/s]\u001b[A\n"," 45% 113/251 [00:16<00:19,  7.21it/s]\u001b[A\n"," 45% 114/251 [00:16<00:18,  7.42it/s]\u001b[A\n"," 46% 115/251 [00:16<00:18,  7.42it/s]\u001b[A\n"," 46% 116/251 [00:16<00:17,  7.51it/s]\u001b[A\n"," 47% 117/251 [00:16<00:17,  7.56it/s]\u001b[A\n"," 47% 118/251 [00:16<00:18,  7.21it/s]\u001b[A\n"," 47% 119/251 [00:17<00:18,  7.03it/s]\u001b[A\n"," 48% 120/251 [00:17<00:18,  7.17it/s]\u001b[A\n"," 48% 121/251 [00:17<00:17,  7.32it/s]\u001b[A\n"," 49% 122/251 [00:17<00:18,  6.99it/s]\u001b[A\n"," 49% 123/251 [00:17<00:18,  6.98it/s]\u001b[A\n"," 49% 124/251 [00:17<00:17,  7.21it/s]\u001b[A\n"," 50% 125/251 [00:17<00:17,  7.32it/s]\u001b[A\n"," 50% 126/251 [00:18<00:17,  7.33it/s]\u001b[A\n"," 51% 127/251 [00:18<00:16,  7.36it/s]\u001b[A\n"," 51% 128/251 [00:18<00:17,  6.96it/s]\u001b[A\n"," 51% 129/251 [00:18<00:18,  6.76it/s]\u001b[A\n"," 52% 130/251 [00:18<00:17,  6.96it/s]\u001b[A\n"," 52% 131/251 [00:18<00:16,  7.16it/s]\u001b[A\n"," 53% 132/251 [00:18<00:16,  7.33it/s]\u001b[A\n"," 53% 133/251 [00:19<00:17,  6.89it/s]\u001b[A\n"," 53% 134/251 [00:19<00:16,  7.03it/s]\u001b[A\n"," 54% 135/251 [00:19<00:16,  7.16it/s]\u001b[A\n"," 54% 136/251 [00:19<00:15,  7.28it/s]\u001b[A\n"," 55% 137/251 [00:19<00:16,  7.02it/s]\u001b[A\n"," 55% 138/251 [00:19<00:15,  7.14it/s]\u001b[A\n"," 55% 139/251 [00:19<00:16,  6.91it/s]\u001b[A\n"," 56% 140/251 [00:20<00:15,  7.21it/s]\u001b[A\n"," 56% 141/251 [00:20<00:15,  7.11it/s]\u001b[A\n"," 57% 142/251 [00:20<00:15,  7.26it/s]\u001b[A\n"," 57% 143/251 [00:20<00:14,  7.32it/s]\u001b[A\n"," 57% 144/251 [00:20<00:15,  6.84it/s]\u001b[A\n"," 58% 145/251 [00:20<00:15,  6.97it/s]\u001b[A\n"," 58% 146/251 [00:20<00:14,  7.14it/s]\u001b[A\n"," 59% 147/251 [00:21<00:14,  7.31it/s]\u001b[A\n"," 59% 148/251 [00:21<00:13,  7.42it/s]\u001b[A\n"," 59% 149/251 [00:21<00:13,  7.44it/s]\u001b[A\n"," 60% 150/251 [00:21<00:13,  7.36it/s]\u001b[A\n"," 60% 151/251 [00:21<00:14,  6.91it/s]\u001b[A\n"," 61% 152/251 [00:21<00:15,  6.57it/s]\u001b[A\n"," 61% 153/251 [00:21<00:16,  6.06it/s]\u001b[A\n"," 61% 154/251 [00:22<00:15,  6.34it/s]\u001b[A\n"," 62% 155/251 [00:22<00:14,  6.53it/s]\u001b[A\n"," 62% 156/251 [00:22<00:14,  6.77it/s]\u001b[A\n"," 63% 157/251 [00:22<00:13,  6.95it/s]\u001b[A\n"," 63% 158/251 [00:22<00:15,  6.17it/s]\u001b[A\n"," 63% 159/251 [00:22<00:14,  6.34it/s]\u001b[A\n"," 64% 160/251 [00:23<00:14,  6.48it/s]\u001b[A\n"," 64% 161/251 [00:23<00:13,  6.64it/s]\u001b[A\n"," 65% 162/251 [00:23<00:13,  6.75it/s]\u001b[A\n"," 65% 163/251 [00:23<00:13,  6.61it/s]\u001b[A\n"," 65% 164/251 [00:23<00:13,  6.69it/s]\u001b[A\n"," 66% 165/251 [00:23<00:12,  6.85it/s]\u001b[A\n"," 66% 166/251 [00:23<00:12,  7.02it/s]\u001b[A\n"," 67% 167/251 [00:23<00:11,  7.24it/s]\u001b[A\n"," 67% 168/251 [00:24<00:12,  6.87it/s]\u001b[A\n"," 67% 169/251 [00:24<00:11,  6.95it/s]\u001b[A\n"," 68% 170/251 [00:24<00:11,  6.81it/s]\u001b[A\n"," 68% 171/251 [00:24<00:12,  6.24it/s]\u001b[A\n"," 69% 172/251 [00:24<00:13,  6.07it/s]\u001b[A\n"," 69% 173/251 [00:24<00:12,  6.43it/s]\u001b[A\n"," 69% 174/251 [00:25<00:12,  6.02it/s]\u001b[A\n"," 70% 175/251 [00:25<00:11,  6.38it/s]\u001b[A\n"," 70% 176/251 [00:25<00:11,  6.66it/s]\u001b[A\n"," 71% 177/251 [00:25<00:10,  7.10it/s]\u001b[A\n"," 71% 178/251 [00:25<00:10,  7.09it/s]\u001b[A\n"," 71% 179/251 [00:25<00:10,  7.18it/s]\u001b[A\n"," 72% 180/251 [00:25<00:09,  7.39it/s]\u001b[A\n"," 72% 181/251 [00:26<00:09,  7.44it/s]\u001b[A\n"," 73% 182/251 [00:26<00:09,  7.45it/s]\u001b[A\n"," 73% 183/251 [00:26<00:09,  7.41it/s]\u001b[A\n"," 73% 184/251 [00:26<00:09,  7.40it/s]\u001b[A\n"," 74% 185/251 [00:26<00:08,  7.34it/s]\u001b[A\n"," 74% 186/251 [00:26<00:08,  7.53it/s]\u001b[A\n"," 75% 187/251 [00:26<00:08,  7.31it/s]\u001b[A\n"," 75% 188/251 [00:27<00:09,  6.56it/s]\u001b[A\n"," 75% 189/251 [00:27<00:09,  6.85it/s]\u001b[A\n"," 76% 190/251 [00:27<00:08,  7.11it/s]\u001b[A\n"," 76% 191/251 [00:27<00:08,  6.84it/s]\u001b[A\n"," 76% 192/251 [00:27<00:08,  7.05it/s]\u001b[A\n"," 77% 193/251 [00:27<00:08,  7.07it/s]\u001b[A\n"," 77% 194/251 [00:27<00:07,  7.20it/s]\u001b[A\n"," 78% 195/251 [00:28<00:08,  6.77it/s]\u001b[A\n"," 78% 196/251 [00:28<00:07,  7.05it/s]\u001b[A\n"," 78% 197/251 [00:28<00:07,  6.81it/s]\u001b[A\n"," 79% 198/251 [00:28<00:07,  7.00it/s]\u001b[A\n"," 79% 199/251 [00:28<00:07,  7.14it/s]\u001b[A\n"," 80% 200/251 [00:28<00:07,  7.27it/s]\u001b[A\n"," 80% 201/251 [00:28<00:07,  6.68it/s]\u001b[A\n"," 80% 202/251 [00:29<00:07,  6.82it/s]\u001b[A\n"," 81% 203/251 [00:29<00:06,  7.12it/s]\u001b[A\n"," 81% 204/251 [00:29<00:06,  7.32it/s]\u001b[A\n"," 82% 205/251 [00:29<00:06,  7.01it/s]\u001b[A\n"," 82% 206/251 [00:29<00:06,  7.14it/s]\u001b[A\n"," 82% 207/251 [00:29<00:06,  6.86it/s]\u001b[A\n"," 83% 208/251 [00:29<00:06,  7.08it/s]\u001b[A\n"," 83% 209/251 [00:30<00:05,  7.22it/s]\u001b[A\n"," 84% 210/251 [00:30<00:05,  7.37it/s]\u001b[A\n"," 84% 211/251 [00:30<00:05,  7.47it/s]\u001b[A\n"," 84% 212/251 [00:30<00:05,  7.54it/s]\u001b[A\n"," 85% 213/251 [00:30<00:05,  7.46it/s]\u001b[A\n"," 85% 214/251 [00:30<00:04,  7.51it/s]\u001b[A\n"," 86% 215/251 [00:30<00:04,  7.53it/s]\u001b[A\n"," 86% 216/251 [00:30<00:04,  7.03it/s]\u001b[A\n"," 86% 217/251 [00:31<00:04,  7.20it/s]\u001b[A\n"," 87% 218/251 [00:31<00:04,  7.34it/s]\u001b[A\n"," 87% 219/251 [00:31<00:04,  7.44it/s]\u001b[A\n"," 88% 220/251 [00:31<00:04,  7.41it/s]\u001b[A\n"," 88% 221/251 [00:31<00:04,  7.45it/s]\u001b[A\n"," 88% 222/251 [00:31<00:03,  7.73it/s]\u001b[A\n"," 89% 223/251 [00:31<00:03,  7.17it/s]\u001b[A\n"," 89% 224/251 [00:32<00:03,  6.85it/s]\u001b[A\n"," 90% 225/251 [00:32<00:04,  5.77it/s]\u001b[A\n"," 90% 226/251 [00:32<00:04,  6.21it/s]\u001b[A\n"," 90% 227/251 [00:32<00:03,  6.57it/s]\u001b[A\n"," 91% 228/251 [00:32<00:03,  6.87it/s]\u001b[A\n"," 91% 229/251 [00:32<00:03,  7.13it/s]\u001b[A\n"," 92% 230/251 [00:32<00:02,  7.17it/s]\u001b[A\n"," 92% 231/251 [00:33<00:02,  7.24it/s]\u001b[A\n"," 92% 232/251 [00:33<00:02,  7.23it/s]\u001b[A\n"," 93% 233/251 [00:33<00:02,  7.27it/s]\u001b[A\n"," 93% 234/251 [00:33<00:02,  7.31it/s]\u001b[A\n"," 94% 235/251 [00:33<00:02,  7.36it/s]\u001b[A\n"," 94% 236/251 [00:33<00:02,  7.35it/s]\u001b[A\n"," 94% 237/251 [00:33<00:01,  7.44it/s]\u001b[A\n"," 95% 238/251 [00:34<00:01,  7.51it/s]\u001b[A\n"," 95% 239/251 [00:34<00:01,  7.63it/s]\u001b[A\n"," 96% 240/251 [00:34<00:01,  7.68it/s]\u001b[A\n"," 96% 241/251 [00:34<00:01,  7.66it/s]\u001b[A\n"," 96% 242/251 [00:34<00:01,  7.66it/s]\u001b[A\n"," 97% 243/251 [00:34<00:01,  7.55it/s]\u001b[A\n"," 97% 244/251 [00:34<00:00,  7.35it/s]\u001b[A\n"," 98% 245/251 [00:35<00:00,  7.08it/s]\u001b[A\n"," 98% 246/251 [00:35<00:00,  7.17it/s]\u001b[A\n"," 98% 247/251 [00:35<00:00,  7.51it/s]\u001b[A\n"," 99% 248/251 [00:35<00:00,  7.55it/s]\u001b[A\n"," 99% 249/251 [00:35<00:00,  7.57it/s]\u001b[A\n","100% 250/251 [00:35<00:00,  7.64it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.019521871581673622, 'eval_runtime': 36.1731, 'eval_samples_per_second': 6.939, 'eval_steps_per_second': 6.939, 'epoch': 1.45}\n"," 48% 200/414 [26:08<26:53,  7.54s/it]\n","100% 251/251 [00:35<00:00,  6.05it/s]\u001b[A\n","                                     \u001b[A[INFO|trainer.py:4309] 2025-12-22 12:10:31,170 >> Saving model checkpoint to /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-200\n","[INFO|configuration_utils.py:765] 2025-12-22 12:10:31,476 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 12:10:31,478 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:10:31,665 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-200/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:10:31,672 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:10:31,677 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-200/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:10:32,648 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:10:32,657 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:10:32,667 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/special_tokens_map.json\n","{'loss': 0.004, 'grad_norm': 0.06404772400856018, 'learning_rate': 5.798850166360461e-05, 'epoch': 1.52}\n","{'loss': 0.0079, 'grad_norm': 0.03644704818725586, 'learning_rate': 5.379665571126231e-05, 'epoch': 1.6}\n","{'loss': 0.005, 'grad_norm': 0.0719616636633873, 'learning_rate': 4.957774794214508e-05, 'epoch': 1.67}\n","{'loss': 0.0043, 'grad_norm': 0.042634159326553345, 'learning_rate': 4.5361849902824674e-05, 'epoch': 1.74}\n","{'loss': 0.0045, 'grad_norm': 0.008854551240801811, 'learning_rate': 4.117901168710959e-05, 'epoch': 1.82}\n","{'loss': 0.0077, 'grad_norm': 0.11988446116447449, 'learning_rate': 3.705904774487396e-05, 'epoch': 1.89}\n","{'loss': 0.0028, 'grad_norm': 0.056600768119096756, 'learning_rate': 3.3031324370510394e-05, 'epoch': 1.96}\n","{'loss': 0.0047, 'grad_norm': 0.01519332081079483, 'learning_rate': 2.9124550385746857e-05, 'epoch': 2.03}\n","{'loss': 0.0037, 'grad_norm': 0.07546248286962509, 'learning_rate': 2.5366572508799886e-05, 'epoch': 2.1}\n","{'loss': 0.0021, 'grad_norm': 0.06585156917572021, 'learning_rate': 2.1784176868432376e-05, 'epoch': 2.17}\n"," 72% 300/414 [38:37<14:09,  7.45s/it][INFO|trainer.py:4643] 2025-12-22 12:23:00,978 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4645] 2025-12-22 12:23:00,979 >>   Num examples = 251\n","[INFO|trainer.py:4648] 2025-12-22 12:23:00,979 >>   Batch size = 1\n","\n","  0% 0/251 [00:00<?, ?it/s]\u001b[A\n","  1% 2/251 [00:00<00:25,  9.65it/s]\u001b[A\n","  1% 3/251 [00:00<00:28,  8.85it/s]\u001b[A\n","  2% 4/251 [00:00<00:29,  8.39it/s]\u001b[A\n","  2% 5/251 [00:00<00:32,  7.49it/s]\u001b[A\n","  2% 6/251 [00:00<00:33,  7.34it/s]\u001b[A\n","  3% 7/251 [00:00<00:31,  7.79it/s]\u001b[A\n","  3% 8/251 [00:01<00:31,  7.74it/s]\u001b[A\n","  4% 9/251 [00:01<00:33,  7.15it/s]\u001b[A\n","  4% 10/251 [00:01<00:33,  7.10it/s]\u001b[A\n","  4% 11/251 [00:01<00:36,  6.51it/s]\u001b[A\n","  5% 12/251 [00:01<00:35,  6.79it/s]\u001b[A\n","  5% 13/251 [00:01<00:34,  6.96it/s]\u001b[A\n","  6% 14/251 [00:01<00:35,  6.70it/s]\u001b[A\n","  6% 15/251 [00:02<00:35,  6.71it/s]\u001b[A\n","  6% 16/251 [00:02<00:33,  6.95it/s]\u001b[A\n","  7% 17/251 [00:02<00:33,  7.06it/s]\u001b[A\n","  7% 18/251 [00:02<00:32,  7.22it/s]\u001b[A\n","  8% 19/251 [00:02<00:31,  7.32it/s]\u001b[A\n","  8% 20/251 [00:02<00:33,  6.90it/s]\u001b[A\n","  8% 21/251 [00:02<00:32,  7.15it/s]\u001b[A\n","  9% 22/251 [00:03<00:31,  7.31it/s]\u001b[A\n","  9% 23/251 [00:03<00:32,  6.98it/s]\u001b[A\n"," 10% 24/251 [00:03<00:33,  6.76it/s]\u001b[A\n"," 10% 25/251 [00:03<00:32,  6.96it/s]\u001b[A\n"," 10% 26/251 [00:03<00:34,  6.60it/s]\u001b[A\n"," 11% 27/251 [00:03<00:32,  6.84it/s]\u001b[A\n"," 11% 28/251 [00:03<00:31,  7.02it/s]\u001b[A\n"," 12% 29/251 [00:04<00:32,  6.76it/s]\u001b[A\n"," 12% 30/251 [00:04<00:31,  6.97it/s]\u001b[A\n"," 12% 31/251 [00:04<00:38,  5.78it/s]\u001b[A\n"," 13% 32/251 [00:04<00:35,  6.19it/s]\u001b[A\n"," 13% 33/251 [00:04<00:33,  6.51it/s]\u001b[A\n"," 14% 34/251 [00:04<00:31,  6.83it/s]\u001b[A\n"," 14% 35/251 [00:04<00:30,  7.11it/s]\u001b[A\n"," 14% 36/251 [00:05<00:31,  6.79it/s]\u001b[A\n"," 15% 37/251 [00:05<00:32,  6.60it/s]\u001b[A\n"," 15% 38/251 [00:05<00:31,  6.82it/s]\u001b[A\n"," 16% 39/251 [00:05<00:30,  7.03it/s]\u001b[A\n"," 16% 40/251 [00:05<00:30,  7.01it/s]\u001b[A\n"," 16% 41/251 [00:05<00:29,  7.09it/s]\u001b[A\n"," 17% 42/251 [00:05<00:29,  7.08it/s]\u001b[A\n"," 17% 43/251 [00:06<00:30,  6.80it/s]\u001b[A\n"," 18% 44/251 [00:06<00:30,  6.70it/s]\u001b[A\n"," 18% 45/251 [00:06<00:29,  7.04it/s]\u001b[A\n"," 18% 46/251 [00:06<00:27,  7.33it/s]\u001b[A\n"," 19% 47/251 [00:06<00:27,  7.51it/s]\u001b[A\n"," 19% 48/251 [00:06<00:27,  7.29it/s]\u001b[A\n"," 20% 49/251 [00:06<00:27,  7.42it/s]\u001b[A\n"," 20% 50/251 [00:07<00:26,  7.54it/s]\u001b[A\n"," 20% 51/251 [00:07<00:28,  6.92it/s]\u001b[A\n"," 21% 52/251 [00:07<00:29,  6.78it/s]\u001b[A\n"," 21% 53/251 [00:07<00:29,  6.64it/s]\u001b[A\n"," 22% 54/251 [00:07<00:28,  6.86it/s]\u001b[A\n"," 22% 55/251 [00:07<00:27,  7.04it/s]\u001b[A\n"," 22% 56/251 [00:07<00:27,  7.06it/s]\u001b[A\n"," 23% 57/251 [00:08<00:26,  7.20it/s]\u001b[A\n"," 23% 58/251 [00:08<00:26,  7.19it/s]\u001b[A\n"," 24% 59/251 [00:08<00:27,  7.08it/s]\u001b[A\n"," 24% 60/251 [00:08<00:26,  7.27it/s]\u001b[A\n"," 24% 61/251 [00:08<00:25,  7.54it/s]\u001b[A\n"," 25% 62/251 [00:08<00:24,  7.56it/s]\u001b[A\n"," 25% 63/251 [00:08<00:24,  7.59it/s]\u001b[A\n"," 25% 64/251 [00:09<00:26,  7.03it/s]\u001b[A\n"," 26% 65/251 [00:09<00:25,  7.25it/s]\u001b[A\n"," 26% 66/251 [00:09<00:26,  7.10it/s]\u001b[A\n"," 27% 67/251 [00:09<00:28,  6.43it/s]\u001b[A\n"," 27% 68/251 [00:09<00:28,  6.33it/s]\u001b[A\n"," 27% 69/251 [00:09<00:27,  6.58it/s]\u001b[A\n"," 28% 70/251 [00:09<00:26,  6.77it/s]\u001b[A\n"," 28% 71/251 [00:10<00:25,  7.01it/s]\u001b[A\n"," 29% 72/251 [00:10<00:25,  7.04it/s]\u001b[A\n"," 29% 73/251 [00:10<00:26,  6.81it/s]\u001b[A\n"," 29% 74/251 [00:10<00:28,  6.13it/s]\u001b[A\n"," 30% 75/251 [00:10<00:27,  6.39it/s]\u001b[A\n"," 30% 76/251 [00:10<00:26,  6.51it/s]\u001b[A\n"," 31% 77/251 [00:11<00:26,  6.61it/s]\u001b[A\n"," 31% 78/251 [00:11<00:25,  6.79it/s]\u001b[A\n"," 31% 79/251 [00:11<00:25,  6.75it/s]\u001b[A\n"," 32% 80/251 [00:11<00:24,  7.00it/s]\u001b[A\n"," 32% 81/251 [00:11<00:23,  7.17it/s]\u001b[A\n"," 33% 82/251 [00:11<00:25,  6.68it/s]\u001b[A\n"," 33% 83/251 [00:11<00:26,  6.44it/s]\u001b[A\n"," 33% 84/251 [00:12<00:25,  6.60it/s]\u001b[A\n"," 34% 85/251 [00:12<00:25,  6.58it/s]\u001b[A\n"," 34% 86/251 [00:12<00:25,  6.37it/s]\u001b[A\n"," 35% 87/251 [00:12<00:25,  6.44it/s]\u001b[A\n"," 35% 88/251 [00:12<00:24,  6.67it/s]\u001b[A\n"," 35% 89/251 [00:12<00:23,  6.90it/s]\u001b[A\n"," 36% 90/251 [00:12<00:24,  6.63it/s]\u001b[A\n"," 36% 91/251 [00:13<00:23,  6.84it/s]\u001b[A\n"," 37% 92/251 [00:13<00:22,  7.04it/s]\u001b[A\n"," 37% 93/251 [00:13<00:22,  7.16it/s]\u001b[A\n"," 37% 94/251 [00:13<00:22,  7.13it/s]\u001b[A\n"," 38% 95/251 [00:13<00:22,  6.98it/s]\u001b[A\n"," 38% 96/251 [00:13<00:23,  6.72it/s]\u001b[A\n"," 39% 97/251 [00:14<00:23,  6.55it/s]\u001b[A\n"," 39% 98/251 [00:14<00:22,  6.81it/s]\u001b[A\n"," 39% 99/251 [00:14<00:22,  6.67it/s]\u001b[A\n"," 40% 100/251 [00:14<00:21,  6.86it/s]\u001b[A\n"," 40% 101/251 [00:14<00:20,  7.18it/s]\u001b[A\n"," 41% 102/251 [00:14<00:21,  6.90it/s]\u001b[A\n"," 41% 103/251 [00:14<00:20,  7.06it/s]\u001b[A\n"," 41% 104/251 [00:14<00:21,  6.96it/s]\u001b[A\n"," 42% 105/251 [00:15<00:21,  6.76it/s]\u001b[A\n"," 42% 106/251 [00:15<00:21,  6.60it/s]\u001b[A\n"," 43% 107/251 [00:15<00:21,  6.80it/s]\u001b[A\n"," 43% 108/251 [00:15<00:20,  6.88it/s]\u001b[A\n"," 43% 109/251 [00:15<00:20,  7.09it/s]\u001b[A\n"," 44% 110/251 [00:15<00:20,  6.94it/s]\u001b[A\n"," 44% 111/251 [00:16<00:21,  6.67it/s]\u001b[A\n"," 45% 112/251 [00:16<00:20,  6.92it/s]\u001b[A\n"," 45% 113/251 [00:16<00:19,  7.18it/s]\u001b[A\n"," 45% 114/251 [00:16<00:18,  7.41it/s]\u001b[A\n"," 46% 115/251 [00:16<00:18,  7.47it/s]\u001b[A\n"," 46% 116/251 [00:16<00:18,  7.48it/s]\u001b[A\n"," 47% 117/251 [00:16<00:17,  7.54it/s]\u001b[A\n"," 47% 118/251 [00:16<00:18,  7.20it/s]\u001b[A\n"," 47% 119/251 [00:17<00:18,  7.04it/s]\u001b[A\n"," 48% 120/251 [00:17<00:18,  7.23it/s]\u001b[A\n"," 48% 121/251 [00:17<00:17,  7.34it/s]\u001b[A\n"," 49% 122/251 [00:17<00:18,  7.01it/s]\u001b[A\n"," 49% 123/251 [00:17<00:18,  6.97it/s]\u001b[A\n"," 49% 124/251 [00:17<00:17,  7.20it/s]\u001b[A\n"," 50% 125/251 [00:17<00:17,  7.31it/s]\u001b[A\n"," 50% 126/251 [00:18<00:17,  7.33it/s]\u001b[A\n"," 51% 127/251 [00:18<00:16,  7.36it/s]\u001b[A\n"," 51% 128/251 [00:18<00:17,  7.01it/s]\u001b[A\n"," 51% 129/251 [00:18<00:18,  6.77it/s]\u001b[A\n"," 52% 130/251 [00:18<00:17,  6.91it/s]\u001b[A\n"," 52% 131/251 [00:18<00:16,  7.11it/s]\u001b[A\n"," 53% 132/251 [00:18<00:16,  7.28it/s]\u001b[A\n"," 53% 133/251 [00:19<00:17,  6.82it/s]\u001b[A\n"," 53% 134/251 [00:19<00:16,  6.96it/s]\u001b[A\n"," 54% 135/251 [00:19<00:16,  7.15it/s]\u001b[A\n"," 54% 136/251 [00:19<00:15,  7.25it/s]\u001b[A\n"," 55% 137/251 [00:19<00:16,  6.97it/s]\u001b[A\n"," 55% 138/251 [00:19<00:16,  6.97it/s]\u001b[A\n"," 55% 139/251 [00:19<00:16,  6.82it/s]\u001b[A\n"," 56% 140/251 [00:20<00:15,  7.07it/s]\u001b[A\n"," 56% 141/251 [00:20<00:15,  6.98it/s]\u001b[A\n"," 57% 142/251 [00:20<00:15,  7.20it/s]\u001b[A\n"," 57% 143/251 [00:20<00:14,  7.28it/s]\u001b[A\n"," 57% 144/251 [00:20<00:15,  6.80it/s]\u001b[A\n"," 58% 145/251 [00:20<00:15,  6.97it/s]\u001b[A\n"," 58% 146/251 [00:20<00:14,  7.17it/s]\u001b[A\n"," 59% 147/251 [00:21<00:14,  7.29it/s]\u001b[A\n"," 59% 148/251 [00:21<00:13,  7.43it/s]\u001b[A\n"," 59% 149/251 [00:21<00:13,  7.38it/s]\u001b[A\n"," 60% 150/251 [00:21<00:13,  7.40it/s]\u001b[A\n"," 60% 151/251 [00:21<00:14,  6.96it/s]\u001b[A\n"," 61% 152/251 [00:21<00:14,  6.62it/s]\u001b[A\n"," 61% 153/251 [00:21<00:16,  6.11it/s]\u001b[A\n"," 61% 154/251 [00:22<00:15,  6.38it/s]\u001b[A\n"," 62% 155/251 [00:22<00:14,  6.66it/s]\u001b[A\n"," 62% 156/251 [00:22<00:13,  6.92it/s]\u001b[A\n"," 63% 157/251 [00:22<00:13,  7.15it/s]\u001b[A\n"," 63% 158/251 [00:22<00:14,  6.29it/s]\u001b[A\n"," 63% 159/251 [00:22<00:14,  6.44it/s]\u001b[A\n"," 64% 160/251 [00:22<00:13,  6.87it/s]\u001b[A\n"," 64% 161/251 [00:23<00:13,  6.92it/s]\u001b[A\n"," 65% 162/251 [00:23<00:12,  6.95it/s]\u001b[A\n"," 65% 163/251 [00:23<00:13,  6.69it/s]\u001b[A\n"," 65% 164/251 [00:23<00:12,  6.75it/s]\u001b[A\n"," 66% 165/251 [00:23<00:12,  6.76it/s]\u001b[A\n"," 66% 166/251 [00:23<00:12,  6.76it/s]\u001b[A\n"," 67% 167/251 [00:24<00:12,  6.95it/s]\u001b[A\n"," 67% 168/251 [00:24<00:11,  7.12it/s]\u001b[A\n"," 67% 169/251 [00:24<00:11,  7.20it/s]\u001b[A\n"," 68% 170/251 [00:24<00:11,  7.29it/s]\u001b[A\n"," 68% 171/251 [00:24<00:12,  6.46it/s]\u001b[A\n"," 69% 172/251 [00:24<00:12,  6.19it/s]\u001b[A\n"," 69% 173/251 [00:24<00:12,  6.33it/s]\u001b[A\n"," 69% 174/251 [00:25<00:12,  5.99it/s]\u001b[A\n"," 70% 175/251 [00:25<00:12,  6.14it/s]\u001b[A\n"," 70% 176/251 [00:25<00:11,  6.30it/s]\u001b[A\n"," 71% 177/251 [00:25<00:11,  6.56it/s]\u001b[A\n"," 71% 178/251 [00:25<00:11,  6.54it/s]\u001b[A\n"," 71% 179/251 [00:25<00:10,  6.62it/s]\u001b[A\n"," 72% 180/251 [00:25<00:10,  6.93it/s]\u001b[A\n"," 72% 181/251 [00:26<00:09,  7.11it/s]\u001b[A\n"," 73% 182/251 [00:26<00:09,  7.13it/s]\u001b[A\n"," 73% 183/251 [00:26<00:09,  7.12it/s]\u001b[A\n"," 73% 184/251 [00:26<00:09,  7.21it/s]\u001b[A\n"," 74% 185/251 [00:26<00:09,  7.28it/s]\u001b[A\n"," 74% 186/251 [00:26<00:08,  7.52it/s]\u001b[A\n"," 75% 187/251 [00:26<00:08,  7.27it/s]\u001b[A\n"," 75% 188/251 [00:27<00:09,  6.53it/s]\u001b[A\n"," 75% 189/251 [00:27<00:09,  6.78it/s]\u001b[A\n"," 76% 190/251 [00:27<00:08,  7.05it/s]\u001b[A\n"," 76% 191/251 [00:27<00:08,  6.80it/s]\u001b[A\n"," 76% 192/251 [00:27<00:08,  7.01it/s]\u001b[A\n"," 77% 193/251 [00:27<00:08,  7.13it/s]\u001b[A\n"," 77% 194/251 [00:27<00:07,  7.23it/s]\u001b[A\n"," 78% 195/251 [00:28<00:08,  6.80it/s]\u001b[A\n"," 78% 196/251 [00:28<00:07,  7.06it/s]\u001b[A\n"," 78% 197/251 [00:28<00:07,  6.80it/s]\u001b[A\n"," 79% 198/251 [00:28<00:07,  7.02it/s]\u001b[A\n"," 79% 199/251 [00:28<00:07,  7.19it/s]\u001b[A\n"," 80% 200/251 [00:28<00:06,  7.30it/s]\u001b[A\n"," 80% 201/251 [00:28<00:07,  6.76it/s]\u001b[A\n"," 80% 202/251 [00:29<00:07,  6.90it/s]\u001b[A\n"," 81% 203/251 [00:29<00:06,  7.16it/s]\u001b[A\n"," 81% 204/251 [00:29<00:06,  7.34it/s]\u001b[A\n"," 82% 205/251 [00:29<00:06,  7.02it/s]\u001b[A\n"," 82% 206/251 [00:29<00:06,  7.15it/s]\u001b[A\n"," 82% 207/251 [00:29<00:06,  6.83it/s]\u001b[A\n"," 83% 208/251 [00:29<00:06,  7.06it/s]\u001b[A\n"," 83% 209/251 [00:30<00:05,  7.24it/s]\u001b[A\n"," 84% 210/251 [00:30<00:05,  7.31it/s]\u001b[A\n"," 84% 211/251 [00:30<00:05,  7.46it/s]\u001b[A\n"," 84% 212/251 [00:30<00:05,  7.55it/s]\u001b[A\n"," 85% 213/251 [00:30<00:05,  7.42it/s]\u001b[A\n"," 85% 214/251 [00:30<00:04,  7.45it/s]\u001b[A\n"," 86% 215/251 [00:30<00:04,  7.51it/s]\u001b[A\n"," 86% 216/251 [00:31<00:05,  6.99it/s]\u001b[A\n"," 86% 217/251 [00:31<00:04,  7.18it/s]\u001b[A\n"," 87% 218/251 [00:31<00:04,  7.35it/s]\u001b[A\n"," 87% 219/251 [00:31<00:04,  7.44it/s]\u001b[A\n"," 88% 220/251 [00:31<00:04,  7.39it/s]\u001b[A\n"," 88% 221/251 [00:31<00:04,  7.40it/s]\u001b[A\n"," 88% 222/251 [00:31<00:03,  7.66it/s]\u001b[A\n"," 89% 223/251 [00:31<00:03,  7.17it/s]\u001b[A\n"," 89% 224/251 [00:32<00:03,  6.85it/s]\u001b[A\n"," 90% 225/251 [00:32<00:04,  5.78it/s]\u001b[A\n"," 90% 226/251 [00:32<00:04,  6.21it/s]\u001b[A\n"," 90% 227/251 [00:32<00:03,  6.54it/s]\u001b[A\n"," 91% 228/251 [00:32<00:03,  6.84it/s]\u001b[A\n"," 91% 229/251 [00:32<00:03,  7.03it/s]\u001b[A\n"," 92% 230/251 [00:33<00:02,  7.10it/s]\u001b[A\n"," 92% 231/251 [00:33<00:02,  7.14it/s]\u001b[A\n"," 92% 232/251 [00:33<00:02,  7.19it/s]\u001b[A\n"," 93% 233/251 [00:33<00:02,  7.15it/s]\u001b[A\n"," 93% 234/251 [00:33<00:02,  7.25it/s]\u001b[A\n"," 94% 235/251 [00:33<00:02,  7.28it/s]\u001b[A\n"," 94% 236/251 [00:33<00:02,  7.17it/s]\u001b[A\n"," 94% 237/251 [00:34<00:01,  7.35it/s]\u001b[A\n"," 95% 238/251 [00:34<00:01,  7.46it/s]\u001b[A\n"," 95% 239/251 [00:34<00:01,  7.51it/s]\u001b[A\n"," 96% 240/251 [00:34<00:01,  7.62it/s]\u001b[A\n"," 96% 241/251 [00:34<00:01,  7.59it/s]\u001b[A\n"," 96% 242/251 [00:34<00:01,  7.57it/s]\u001b[A\n"," 97% 243/251 [00:34<00:01,  7.53it/s]\u001b[A\n"," 97% 244/251 [00:34<00:00,  7.48it/s]\u001b[A\n"," 98% 245/251 [00:35<00:00,  7.46it/s]\u001b[A\n"," 98% 246/251 [00:35<00:00,  7.55it/s]\u001b[A\n"," 98% 247/251 [00:35<00:00,  7.74it/s]\u001b[A\n"," 99% 248/251 [00:35<00:00,  7.75it/s]\u001b[A\n"," 99% 249/251 [00:35<00:00,  7.74it/s]\u001b[A\n","100% 250/251 [00:35<00:00,  7.72it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.009713967330753803, 'eval_runtime': 36.2354, 'eval_samples_per_second': 6.927, 'eval_steps_per_second': 6.927, 'epoch': 2.17}\n"," 72% 300/414 [39:14<14:09,  7.45s/it]\n","100% 251/251 [00:35<00:00,  6.20it/s]\u001b[A\n","{'loss': 0.0042, 'grad_norm': 0.009845152497291565, 'learning_rate': 1.8402898077684804e-05, 'epoch': 2.25}\n","{'loss': 0.0029, 'grad_norm': 0.07996924966573715, 'learning_rate': 1.5246837228164907e-05, 'epoch': 2.32}\n","{'loss': 0.0037, 'grad_norm': 0.03723110631108284, 'learning_rate': 1.2338490102196825e-05, 'epoch': 2.39}\n","{'loss': 0.0016, 'grad_norm': 0.010701577179133892, 'learning_rate': 9.69858682729976e-06, 'epoch': 2.47}\n","{'loss': 0.0029, 'grad_norm': 0.05035362020134926, 'learning_rate': 7.345944115907422e-06, 'epoch': 2.54}\n","{'loss': 0.0016, 'grad_norm': 0.04168945923447609, 'learning_rate': 5.2973311435349725e-06, 'epoch': 2.61}\n","{'loss': 0.0037, 'grad_norm': 0.08423133194446564, 'learning_rate': 3.567350021386895e-06, 'epoch': 2.68}\n","{'loss': 0.0026, 'grad_norm': 0.01333162933588028, 'learning_rate': 2.1683317153742776e-06, 'epoch': 2.76}\n","{'loss': 0.0013, 'grad_norm': 0.019100811332464218, 'learning_rate': 1.1102481534098374e-06, 'epoch': 2.83}\n","{'loss': 0.0014, 'grad_norm': 0.026234734803438187, 'learning_rate': 4.006411474628491e-07, 'epoch': 2.9}\n"," 97% 400/414 [51:40<01:44,  7.46s/it][INFO|trainer.py:4643] 2025-12-22 12:36:03,881 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4645] 2025-12-22 12:36:03,881 >>   Num examples = 251\n","[INFO|trainer.py:4648] 2025-12-22 12:36:03,881 >>   Batch size = 1\n","\n","  0% 0/251 [00:00<?, ?it/s]\u001b[A\n","  1% 2/251 [00:00<00:25,  9.67it/s]\u001b[A\n","  1% 3/251 [00:00<00:27,  8.87it/s]\u001b[A\n","  2% 4/251 [00:00<00:29,  8.36it/s]\u001b[A\n","  2% 5/251 [00:00<00:32,  7.48it/s]\u001b[A\n","  2% 6/251 [00:00<00:33,  7.32it/s]\u001b[A\n","  3% 7/251 [00:00<00:31,  7.80it/s]\u001b[A\n","  3% 8/251 [00:01<00:31,  7.77it/s]\u001b[A\n","  4% 9/251 [00:01<00:33,  7.13it/s]\u001b[A\n","  4% 10/251 [00:01<00:33,  7.15it/s]\u001b[A\n","  4% 11/251 [00:01<00:36,  6.53it/s]\u001b[A\n","  5% 12/251 [00:01<00:35,  6.78it/s]\u001b[A\n","  5% 13/251 [00:01<00:34,  6.96it/s]\u001b[A\n","  6% 14/251 [00:01<00:35,  6.72it/s]\u001b[A\n","  6% 15/251 [00:02<00:34,  6.84it/s]\u001b[A\n","  6% 16/251 [00:02<00:33,  7.05it/s]\u001b[A\n","  7% 17/251 [00:02<00:32,  7.11it/s]\u001b[A\n","  7% 18/251 [00:02<00:32,  7.26it/s]\u001b[A\n","  8% 19/251 [00:02<00:31,  7.29it/s]\u001b[A\n","  8% 20/251 [00:02<00:33,  6.90it/s]\u001b[A\n","  8% 21/251 [00:02<00:32,  7.03it/s]\u001b[A\n","  9% 22/251 [00:03<00:31,  7.21it/s]\u001b[A\n","  9% 23/251 [00:03<00:33,  6.88it/s]\u001b[A\n"," 10% 24/251 [00:03<00:33,  6.68it/s]\u001b[A\n"," 10% 25/251 [00:03<00:32,  6.94it/s]\u001b[A\n"," 10% 26/251 [00:03<00:34,  6.60it/s]\u001b[A\n"," 11% 27/251 [00:03<00:32,  6.85it/s]\u001b[A\n"," 11% 28/251 [00:03<00:31,  7.09it/s]\u001b[A\n"," 12% 29/251 [00:04<00:32,  6.87it/s]\u001b[A\n"," 12% 30/251 [00:04<00:31,  7.02it/s]\u001b[A\n"," 12% 31/251 [00:04<00:38,  5.76it/s]\u001b[A\n"," 13% 32/251 [00:04<00:35,  6.19it/s]\u001b[A\n"," 13% 33/251 [00:04<00:33,  6.51it/s]\u001b[A\n"," 14% 34/251 [00:04<00:31,  6.84it/s]\u001b[A\n"," 14% 35/251 [00:04<00:30,  7.06it/s]\u001b[A\n"," 14% 36/251 [00:05<00:31,  6.77it/s]\u001b[A\n"," 15% 37/251 [00:05<00:32,  6.57it/s]\u001b[A\n"," 15% 38/251 [00:05<00:31,  6.80it/s]\u001b[A\n"," 16% 39/251 [00:05<00:30,  7.05it/s]\u001b[A\n"," 16% 40/251 [00:05<00:29,  7.08it/s]\u001b[A\n"," 16% 41/251 [00:05<00:29,  7.19it/s]\u001b[A\n"," 17% 42/251 [00:05<00:29,  7.13it/s]\u001b[A\n"," 17% 43/251 [00:06<00:30,  6.80it/s]\u001b[A\n"," 18% 44/251 [00:06<00:31,  6.67it/s]\u001b[A\n"," 18% 45/251 [00:06<00:29,  6.97it/s]\u001b[A\n"," 18% 46/251 [00:06<00:28,  7.31it/s]\u001b[A\n"," 19% 47/251 [00:06<00:27,  7.50it/s]\u001b[A\n"," 19% 48/251 [00:06<00:27,  7.32it/s]\u001b[A\n"," 20% 49/251 [00:06<00:27,  7.47it/s]\u001b[A\n"," 20% 50/251 [00:07<00:26,  7.54it/s]\u001b[A\n"," 20% 51/251 [00:07<00:28,  7.04it/s]\u001b[A\n"," 21% 52/251 [00:07<00:28,  6.87it/s]\u001b[A\n"," 21% 53/251 [00:07<00:29,  6.70it/s]\u001b[A\n"," 22% 54/251 [00:07<00:28,  6.88it/s]\u001b[A\n"," 22% 55/251 [00:07<00:27,  7.07it/s]\u001b[A\n"," 22% 56/251 [00:07<00:27,  7.05it/s]\u001b[A\n"," 23% 57/251 [00:08<00:26,  7.23it/s]\u001b[A\n"," 23% 58/251 [00:08<00:26,  7.24it/s]\u001b[A\n"," 24% 59/251 [00:08<00:27,  7.11it/s]\u001b[A\n"," 24% 60/251 [00:08<00:26,  7.30it/s]\u001b[A\n"," 24% 61/251 [00:08<00:25,  7.52it/s]\u001b[A\n"," 25% 62/251 [00:08<00:24,  7.57it/s]\u001b[A\n"," 25% 63/251 [00:08<00:24,  7.65it/s]\u001b[A\n"," 25% 64/251 [00:09<00:26,  7.06it/s]\u001b[A\n"," 26% 65/251 [00:09<00:25,  7.26it/s]\u001b[A\n"," 26% 66/251 [00:09<00:25,  7.15it/s]\u001b[A\n"," 27% 67/251 [00:09<00:28,  6.47it/s]\u001b[A\n"," 27% 68/251 [00:09<00:28,  6.42it/s]\u001b[A\n"," 27% 69/251 [00:09<00:27,  6.67it/s]\u001b[A\n"," 28% 70/251 [00:09<00:26,  6.91it/s]\u001b[A\n"," 28% 71/251 [00:10<00:27,  6.65it/s]\u001b[A\n"," 29% 72/251 [00:10<00:26,  6.84it/s]\u001b[A\n"," 29% 73/251 [00:10<00:25,  6.88it/s]\u001b[A\n"," 29% 74/251 [00:10<00:28,  6.19it/s]\u001b[A\n"," 30% 75/251 [00:10<00:27,  6.49it/s]\u001b[A\n"," 30% 76/251 [00:10<00:26,  6.62it/s]\u001b[A\n"," 31% 77/251 [00:11<00:26,  6.66it/s]\u001b[A\n"," 31% 78/251 [00:11<00:26,  6.57it/s]\u001b[A\n"," 31% 79/251 [00:11<00:26,  6.59it/s]\u001b[A\n"," 32% 80/251 [00:11<00:25,  6.63it/s]\u001b[A\n"," 32% 81/251 [00:11<00:24,  6.93it/s]\u001b[A\n"," 33% 82/251 [00:11<00:25,  6.57it/s]\u001b[A\n"," 33% 83/251 [00:11<00:25,  6.52it/s]\u001b[A\n"," 33% 84/251 [00:12<00:25,  6.67it/s]\u001b[A\n"," 34% 85/251 [00:12<00:24,  6.91it/s]\u001b[A\n"," 34% 86/251 [00:12<00:24,  6.67it/s]\u001b[A\n"," 35% 87/251 [00:12<00:25,  6.46it/s]\u001b[A\n"," 35% 88/251 [00:12<00:25,  6.51it/s]\u001b[A\n"," 35% 89/251 [00:12<00:24,  6.53it/s]\u001b[A\n"," 36% 90/251 [00:13<00:25,  6.28it/s]\u001b[A\n"," 36% 91/251 [00:13<00:24,  6.45it/s]\u001b[A\n"," 37% 92/251 [00:13<00:23,  6.74it/s]\u001b[A\n"," 37% 93/251 [00:13<00:23,  6.81it/s]\u001b[A\n"," 37% 94/251 [00:13<00:22,  6.84it/s]\u001b[A\n"," 38% 95/251 [00:13<00:22,  6.84it/s]\u001b[A\n"," 38% 96/251 [00:13<00:23,  6.61it/s]\u001b[A\n"," 39% 97/251 [00:14<00:23,  6.51it/s]\u001b[A\n"," 39% 98/251 [00:14<00:22,  6.76it/s]\u001b[A\n"," 39% 99/251 [00:14<00:23,  6.61it/s]\u001b[A\n"," 40% 100/251 [00:14<00:22,  6.82it/s]\u001b[A\n"," 40% 101/251 [00:14<00:20,  7.20it/s]\u001b[A\n"," 41% 102/251 [00:14<00:21,  6.88it/s]\u001b[A\n"," 41% 103/251 [00:14<00:20,  7.05it/s]\u001b[A\n"," 41% 104/251 [00:15<00:21,  6.97it/s]\u001b[A\n"," 42% 105/251 [00:15<00:21,  6.75it/s]\u001b[A\n"," 42% 106/251 [00:15<00:22,  6.59it/s]\u001b[A\n"," 43% 107/251 [00:15<00:21,  6.83it/s]\u001b[A\n"," 43% 108/251 [00:15<00:20,  7.01it/s]\u001b[A\n"," 43% 109/251 [00:15<00:19,  7.21it/s]\u001b[A\n"," 44% 110/251 [00:15<00:20,  7.02it/s]\u001b[A\n"," 44% 111/251 [00:16<00:20,  6.71it/s]\u001b[A\n"," 45% 112/251 [00:16<00:19,  6.98it/s]\u001b[A\n"," 45% 113/251 [00:16<00:19,  7.22it/s]\u001b[A\n"," 45% 114/251 [00:16<00:18,  7.50it/s]\u001b[A\n"," 46% 115/251 [00:16<00:18,  7.55it/s]\u001b[A\n"," 46% 116/251 [00:16<00:17,  7.58it/s]\u001b[A\n"," 47% 117/251 [00:16<00:17,  7.62it/s]\u001b[A\n"," 47% 118/251 [00:16<00:18,  7.25it/s]\u001b[A\n"," 47% 119/251 [00:17<00:18,  7.06it/s]\u001b[A\n"," 48% 120/251 [00:17<00:18,  7.20it/s]\u001b[A\n"," 48% 121/251 [00:17<00:17,  7.38it/s]\u001b[A\n"," 49% 122/251 [00:17<00:18,  7.03it/s]\u001b[A\n"," 49% 123/251 [00:17<00:18,  7.04it/s]\u001b[A\n"," 49% 124/251 [00:17<00:17,  7.24it/s]\u001b[A\n"," 50% 125/251 [00:17<00:17,  7.32it/s]\u001b[A\n"," 50% 126/251 [00:18<00:17,  7.32it/s]\u001b[A\n"," 51% 127/251 [00:18<00:16,  7.32it/s]\u001b[A\n"," 51% 128/251 [00:18<00:17,  6.99it/s]\u001b[A\n"," 51% 129/251 [00:18<00:17,  6.79it/s]\u001b[A\n"," 52% 130/251 [00:18<00:17,  6.95it/s]\u001b[A\n"," 52% 131/251 [00:18<00:16,  7.11it/s]\u001b[A\n"," 53% 132/251 [00:18<00:16,  7.28it/s]\u001b[A\n"," 53% 133/251 [00:19<00:17,  6.83it/s]\u001b[A\n"," 53% 134/251 [00:19<00:16,  6.93it/s]\u001b[A\n"," 54% 135/251 [00:19<00:16,  7.13it/s]\u001b[A\n"," 54% 136/251 [00:19<00:15,  7.27it/s]\u001b[A\n"," 55% 137/251 [00:19<00:16,  6.96it/s]\u001b[A\n"," 55% 138/251 [00:19<00:15,  7.13it/s]\u001b[A\n"," 55% 139/251 [00:19<00:16,  6.95it/s]\u001b[A\n"," 56% 140/251 [00:20<00:15,  7.15it/s]\u001b[A\n"," 56% 141/251 [00:20<00:15,  7.06it/s]\u001b[A\n"," 57% 142/251 [00:20<00:15,  7.23it/s]\u001b[A\n"," 57% 143/251 [00:20<00:14,  7.35it/s]\u001b[A\n"," 57% 144/251 [00:20<00:15,  6.84it/s]\u001b[A\n"," 58% 145/251 [00:20<00:15,  6.98it/s]\u001b[A\n"," 58% 146/251 [00:20<00:14,  7.15it/s]\u001b[A\n"," 59% 147/251 [00:21<00:14,  7.34it/s]\u001b[A\n"," 59% 148/251 [00:21<00:13,  7.45it/s]\u001b[A\n"," 59% 149/251 [00:21<00:13,  7.46it/s]\u001b[A\n"," 60% 150/251 [00:21<00:13,  7.44it/s]\u001b[A\n"," 60% 151/251 [00:21<00:14,  6.99it/s]\u001b[A\n"," 61% 152/251 [00:21<00:15,  6.54it/s]\u001b[A\n"," 61% 153/251 [00:22<00:16,  5.93it/s]\u001b[A\n"," 61% 154/251 [00:22<00:15,  6.31it/s]\u001b[A\n"," 62% 155/251 [00:22<00:14,  6.56it/s]\u001b[A\n"," 62% 156/251 [00:22<00:13,  6.86it/s]\u001b[A\n"," 63% 157/251 [00:22<00:13,  7.09it/s]\u001b[A\n"," 63% 158/251 [00:22<00:14,  6.21it/s]\u001b[A\n"," 63% 159/251 [00:22<00:14,  6.54it/s]\u001b[A\n"," 64% 160/251 [00:23<00:13,  6.82it/s]\u001b[A\n"," 64% 161/251 [00:23<00:12,  6.95it/s]\u001b[A\n"," 65% 162/251 [00:23<00:13,  6.82it/s]\u001b[A\n"," 65% 163/251 [00:23<00:13,  6.69it/s]\u001b[A\n"," 65% 164/251 [00:23<00:12,  6.87it/s]\u001b[A\n"," 66% 165/251 [00:23<00:12,  6.89it/s]\u001b[A\n"," 66% 166/251 [00:23<00:11,  7.09it/s]\u001b[A\n"," 67% 167/251 [00:24<00:12,  6.91it/s]\u001b[A\n"," 67% 168/251 [00:24<00:11,  6.98it/s]\u001b[A\n"," 67% 169/251 [00:24<00:11,  6.92it/s]\u001b[A\n"," 68% 170/251 [00:24<00:11,  6.98it/s]\u001b[A\n"," 68% 171/251 [00:24<00:12,  6.32it/s]\u001b[A\n"," 69% 172/251 [00:24<00:12,  6.12it/s]\u001b[A\n"," 69% 173/251 [00:24<00:12,  6.37it/s]\u001b[A\n"," 69% 174/251 [00:25<00:12,  6.01it/s]\u001b[A\n"," 70% 175/251 [00:25<00:12,  6.32it/s]\u001b[A\n"," 70% 176/251 [00:25<00:11,  6.64it/s]\u001b[A\n"," 71% 177/251 [00:25<00:10,  7.05it/s]\u001b[A\n"," 71% 178/251 [00:25<00:10,  6.77it/s]\u001b[A\n"," 71% 179/251 [00:25<00:10,  6.75it/s]\u001b[A\n"," 72% 180/251 [00:26<00:10,  6.77it/s]\u001b[A\n"," 72% 181/251 [00:26<00:10,  6.56it/s]\u001b[A\n"," 73% 182/251 [00:26<00:10,  6.51it/s]\u001b[A\n"," 73% 183/251 [00:26<00:10,  6.69it/s]\u001b[A\n"," 73% 184/251 [00:26<00:09,  6.90it/s]\u001b[A\n"," 74% 185/251 [00:26<00:09,  7.03it/s]\u001b[A\n"," 74% 186/251 [00:26<00:08,  7.32it/s]\u001b[A\n"," 75% 187/251 [00:27<00:08,  7.19it/s]\u001b[A\n"," 75% 188/251 [00:27<00:09,  6.47it/s]\u001b[A\n"," 75% 189/251 [00:27<00:09,  6.73it/s]\u001b[A\n"," 76% 190/251 [00:27<00:08,  6.97it/s]\u001b[A\n"," 76% 191/251 [00:27<00:08,  6.77it/s]\u001b[A\n"," 76% 192/251 [00:27<00:08,  7.01it/s]\u001b[A\n"," 77% 193/251 [00:27<00:08,  7.17it/s]\u001b[A\n"," 77% 194/251 [00:28<00:07,  7.31it/s]\u001b[A\n"," 78% 195/251 [00:28<00:08,  6.86it/s]\u001b[A\n"," 78% 196/251 [00:28<00:07,  7.04it/s]\u001b[A\n"," 78% 197/251 [00:28<00:08,  6.75it/s]\u001b[A\n"," 79% 198/251 [00:28<00:07,  6.97it/s]\u001b[A\n"," 79% 199/251 [00:28<00:07,  7.11it/s]\u001b[A\n"," 80% 200/251 [00:28<00:07,  7.24it/s]\u001b[A\n"," 80% 201/251 [00:29<00:07,  6.70it/s]\u001b[A\n"," 80% 202/251 [00:29<00:07,  6.88it/s]\u001b[A\n"," 81% 203/251 [00:29<00:06,  7.15it/s]\u001b[A\n"," 81% 204/251 [00:29<00:06,  7.28it/s]\u001b[A\n"," 82% 205/251 [00:29<00:06,  6.96it/s]\u001b[A\n"," 82% 206/251 [00:29<00:06,  7.12it/s]\u001b[A\n"," 82% 207/251 [00:29<00:06,  6.87it/s]\u001b[A\n"," 83% 208/251 [00:30<00:06,  7.10it/s]\u001b[A\n"," 83% 209/251 [00:30<00:05,  7.24it/s]\u001b[A\n"," 84% 210/251 [00:30<00:05,  7.33it/s]\u001b[A\n"," 84% 211/251 [00:30<00:05,  7.45it/s]\u001b[A\n"," 84% 212/251 [00:30<00:05,  7.51it/s]\u001b[A\n"," 85% 213/251 [00:30<00:05,  7.50it/s]\u001b[A\n"," 85% 214/251 [00:30<00:04,  7.50it/s]\u001b[A\n"," 86% 215/251 [00:30<00:04,  7.53it/s]\u001b[A\n"," 86% 216/251 [00:31<00:04,  7.03it/s]\u001b[A\n"," 86% 217/251 [00:31<00:04,  7.22it/s]\u001b[A\n"," 87% 218/251 [00:31<00:04,  7.37it/s]\u001b[A\n"," 87% 219/251 [00:31<00:04,  7.46it/s]\u001b[A\n"," 88% 220/251 [00:31<00:04,  7.41it/s]\u001b[A\n"," 88% 221/251 [00:31<00:04,  7.39it/s]\u001b[A\n"," 88% 222/251 [00:31<00:03,  7.65it/s]\u001b[A\n"," 89% 223/251 [00:32<00:03,  7.14it/s]\u001b[A\n"," 89% 224/251 [00:32<00:03,  6.84it/s]\u001b[A\n"," 90% 225/251 [00:32<00:04,  5.79it/s]\u001b[A\n"," 90% 226/251 [00:32<00:04,  6.18it/s]\u001b[A\n"," 90% 227/251 [00:32<00:03,  6.60it/s]\u001b[A\n"," 91% 228/251 [00:32<00:03,  6.88it/s]\u001b[A\n"," 91% 229/251 [00:32<00:03,  7.09it/s]\u001b[A\n"," 92% 230/251 [00:33<00:02,  7.16it/s]\u001b[A\n"," 92% 231/251 [00:33<00:02,  7.25it/s]\u001b[A\n"," 92% 232/251 [00:33<00:02,  7.33it/s]\u001b[A\n"," 93% 233/251 [00:33<00:02,  7.24it/s]\u001b[A\n"," 93% 234/251 [00:33<00:02,  7.33it/s]\u001b[A\n"," 94% 235/251 [00:33<00:02,  7.34it/s]\u001b[A\n"," 94% 236/251 [00:33<00:02,  7.30it/s]\u001b[A\n"," 94% 237/251 [00:34<00:01,  7.44it/s]\u001b[A\n"," 95% 238/251 [00:34<00:01,  7.48it/s]\u001b[A\n"," 95% 239/251 [00:34<00:01,  7.61it/s]\u001b[A\n"," 96% 240/251 [00:34<00:01,  7.64it/s]\u001b[A\n"," 96% 241/251 [00:34<00:01,  7.61it/s]\u001b[A\n"," 96% 242/251 [00:34<00:01,  7.62it/s]\u001b[A\n"," 97% 243/251 [00:34<00:01,  7.53it/s]\u001b[A\n"," 97% 244/251 [00:34<00:00,  7.47it/s]\u001b[A\n"," 98% 245/251 [00:35<00:00,  7.56it/s]\u001b[A\n"," 98% 246/251 [00:35<00:00,  7.60it/s]\u001b[A\n"," 98% 247/251 [00:35<00:00,  7.88it/s]\u001b[A\n"," 99% 248/251 [00:35<00:00,  7.85it/s]\u001b[A\n"," 99% 249/251 [00:35<00:00,  7.80it/s]\u001b[A\n","100% 250/251 [00:35<00:00,  7.73it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.009813960641622543, 'eval_runtime': 36.3087, 'eval_samples_per_second': 6.913, 'eval_steps_per_second': 6.913, 'epoch': 2.9}\n"," 97% 400/414 [52:17<01:44,  7.46s/it]\n","100% 251/251 [00:35<00:00,  6.98it/s]\u001b[A\n","                                     \u001b[A[INFO|trainer.py:4309] 2025-12-22 12:36:40,197 >> Saving model checkpoint to /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-400\n","[INFO|configuration_utils.py:765] 2025-12-22 12:36:40,455 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 12:36:40,456 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:36:40,659 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-400/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:36:40,673 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-400/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:36:40,684 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-400/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:36:41,911 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:36:41,931 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:36:41,944 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/special_tokens_map.json\n","{'loss': 0.0027, 'grad_norm': 0.03913852572441101, 'learning_rate': 4.456863700363356e-08, 'epoch': 2.98}\n","100% 414/414 [53:59<00:00,  6.03s/it][INFO|trainer.py:4309] 2025-12-22 12:38:22,686 >> Saving model checkpoint to /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-414\n","[INFO|configuration_utils.py:765] 2025-12-22 12:38:22,929 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 12:38:22,930 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:38:23,111 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-414/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:38:23,117 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-414/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:38:23,123 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-414/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:38:24,133 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:38:24,141 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:38:24,146 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/special_tokens_map.json\n","[INFO|trainer.py:4418] 2025-12-22 12:38:24,379 >> Deleting older checkpoint [/content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-200] due to args.save_total_limit\n","[INFO|trainer.py:2810] 2025-12-22 12:38:24,582 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 3241.5041, 'train_samples_per_second': 2.033, 'train_steps_per_second': 0.128, 'train_loss': 0.010833598586735159, 'epoch': 3.0}\n","100% 414/414 [54:01<00:00,  7.83s/it]\n","[INFO|trainer.py:5114] 2025-12-22 12:38:24,588 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n","[INFO|trainer.py:4309] 2025-12-22 12:38:35,081 >> Saving model checkpoint to /content/drive/MyDrive/llm-finetuning/quiz_parser\n","[INFO|configuration_utils.py:765] 2025-12-22 12:38:35,306 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 12:38:35,307 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:38:35,521 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:38:35,527 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:38:35,532 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/special_tokens_map.json\n","[INFO|trainer.py:4309] 2025-12-22 12:38:35,874 >> Saving model checkpoint to /content/drive/MyDrive/llm-finetuning/quiz_parser\n","[INFO|configuration_utils.py:765] 2025-12-22 12:38:36,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 12:38:36,094 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:38:36,294 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:38:36,300 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:38:36,306 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/special_tokens_map.json\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  ...adapter_model.safetensors:  45% 16.8M/37.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (2 / 3)      :  58% 28.2M/48.4M [00:00<00:00, 138MB/s,   ???B/s  ]\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (2 / 3)      :  93% 44.9M/48.4M [00:00<00:00, 106MB/s, 83.5MB/s  ]\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (3 / 3)      : 100% 48.4M/48.4M [00:00<00:00, 66.0MB/s, 50.5MB/s  ]\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  ...adapter_model.safetensors: 100% 37.0M/37.0M [00:00<00:00, 40.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (3 / 3)      : 100% 48.4M/48.4M [00:00<00:00, 60.2MB/s, 33.7MB/s  ]\n","New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\n","  ...adapter_model.safetensors: 100% 37.0M/37.0M [00:00<00:00, 33.6MB/s]\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               = 23752538GF\n","  train_loss               =     0.0108\n","  train_runtime            = 0:54:01.50\n","  train_samples_per_second =      2.033\n","  train_steps_per_second   =      0.128\n","[INFO|trainer.py:4643] 2025-12-22 12:38:40,481 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4645] 2025-12-22 12:38:40,481 >>   Num examples = 251\n","[INFO|trainer.py:4648] 2025-12-22 12:38:40,481 >>   Batch size = 1\n","100% 251/251 [00:36<00:00,  6.96it/s]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_loss               =     0.0098\n","  eval_runtime            = 0:00:36.37\n","  eval_samples_per_second =        6.9\n","  eval_steps_per_second   =        6.9\n","[INFO|trainer.py:4309] 2025-12-22 12:39:16,878 >> Saving model checkpoint to /content/drive/MyDrive/llm-finetuning/quiz_parser\n","[INFO|configuration_utils.py:765] 2025-12-22 12:39:17,122 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:839] 2025-12-22 12:39:17,123 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"transformers_version\": \"4.57.1\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2421] 2025-12-22 12:39:17,344 >> chat template saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2590] 2025-12-22 12:39:17,350 >> tokenizer config file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2599] 2025-12-22 12:39:17,359 >> Special tokens file saved in /content/drive/MyDrive/llm-finetuning/quiz_parser/special_tokens_map.json\n","[INFO|modelcard.py:456] 2025-12-22 12:39:17,766 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json:  73% 8.30M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  ...adapter_model.safetensors:  23% 8.39M/37.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json:  73% 8.30M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (1 / 3)      :  34% 16.7M/48.4M [00:00<00:00, 78.7MB/s,   ???B/s  ]\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 15.9MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (2 / 3)      :  75% 36.5M/48.4M [00:00<00:00, 92.2MB/s, 98.8MB/s  ]\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 7.88MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (3 / 3)      : 100% 48.4M/48.4M [00:00<00:00, 77.5MB/s, 79.2MB/s  ]\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 5.25MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  ...adapter_model.safetensors: 100% 37.0M/37.0M [00:00<00:00, 48.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 4.73MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  ...adapter_model.safetensors: 100% 37.0M/37.0M [00:00<00:00, 43.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 3.93MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (3 / 3)      : 100% 48.4M/48.4M [00:01<00:00, 48.2MB/s, 39.6MB/s  ]\n","New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n","  ..._parser/training_args.bin: 100% 6.29k/6.29k [00:00<?, ?B/s]\n","  ...uiz_parser/tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 3.92MB/s]\n","  ...adapter_model.safetensors: 100% 37.0M/37.0M [00:00<00:00, 36.0MB/s]\n"]}]},{"cell_type":"markdown","source":["#Run The Model with our Layer"],"metadata":{"id":"JhzIHJZHNaFZ"}},{"cell_type":"code","source":["!pip install -qU transformers\n","!pip install -qU json-repair\n","import json\n","import re\n","import json_repair\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","device = \"cuda\"\n","torch_dtype = torch.float16\n","\n","def parse_json(text):\n","    try:\n","        return json_repair.loads(text)\n","    except:\n","        return None\n","\n","def trim_all(text):\n","  try:\n","    text = re.sub(r\" {2,}\", \" \", text)\n","    text = re.sub(r\"[ ]+\\n|\\n[ ]+\", \"\\n\", text)\n","    text = re.sub(r\"\\n{3,}|[ ]*\\n[ ]+\\n\", \"\\n\\n\", text)\n","    text = re.sub(r\"‘|’\", \"'\", text)          # ‘ ’\n","    text = re.sub(r\"“|”\", \"\\\"\", text)         # “ ”\n","    text = re.sub(r\"[–—]\", \"-\", text)         # – —\n","    text = re.sub(r\"[_-]{6,}\", \"_____\", text) # _ -\n","    text = re.sub(r\"[‥…]\", \"..\", text)        # ‥ …\n","    text = re.sub(r\",{6,}\", \".....\", text)    # ,\n","    text = re.sub(r\"\\.{6,}\", \".....\", text)   # .\n","    return text.split(\"\\n\\n\")\n","  except:\n","    return None"],"metadata":{"id":"1Kv1XOeqYBee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    device_map=\"auto\",\n","    torch_dtype = torch_dtype\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"rcEedeI9PBgB","executionInfo":{"status":"ok","timestamp":1767035300779,"user_tz":-180,"elapsed":27318,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"78fc1597-ba25-4008-cf7d-78106aff3ba7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","`torch_dtype` is deprecated! Use `dtype` instead!\n"]}]},{"cell_type":"code","source":["finetuned_model_id = \"/content/drive/MyDrive/llm-finetuning/quiz_parser\"\n","model.load_adapter(finetuned_model_id)"],"metadata":{"collapsed":true,"id":"ZtuNW4udFSmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = \"\"\"\n","1. What is the value of π (pi) approximately equal to?\n","A) 2.14\n","B) 3.14\n","C) 4.13\n","D) 3.41\n","Correct is 3.14.\n","\n","2. Who wrote \"Romeo and Juliet\"?\n","1) Charles Dickens\n","2) **William Shakespeare**\n","3) George Orwell\n","4) Mark Twain\n","\n","\n","\n","3. The square root of 144 is:\n","A) 10\n","B) 11\n","C) 12\n","D) 14\n","Correct Option → c\n","\n","This is an insane amount of knowledge shared for free on the internet, thank you !\n","\n","35. After World War I, in European countries, in the Soviet Union and Japan powerfuldictatorships emerged.\n","a. [highlight]True[/highlight]\n","b. False\n","36........... promised a better future to Germans that were feeling humiliated by losing theWorld War I.\n","a. Benito Musoolini\n","b. [highlight]Adolph Hitler[/highlight]\n","c. Nicholas II\n","d. Peter the Great\n","\n","19. What did Voltaire not support? A) freedom of religion B) freedom of expression C) separation of church and state. **D) State control over the economy**\n","\n","GEP1006 FINAL GROUP A\n","\n","30) ……… was the leader during the radical phase of the Fench Revelation. [/colored]a. louis 14[/colored] b. Danton  c. henry 8 d. Maximilian Robespierre\n","\n","39. which of the  following was among the consequences of World War III\n","A Germany was divided into four regions of administration.\n","B East Germany became under the Soviet control.\n","C the world was divided in two a bi-polar system as Eastern and Western Bloc\n","[highlight]D  All of answers: A , B& C[/highlight]\n","\n","1. Which of the following was the capital of the Lydians?\n","A) Olympia ✅\n","B) Miletus\n","C) Aspendos\n","D) Sardis\n","\n","\n","\n","1- Seismic survey use ...C.. sound waves to produce a a \"CATScan\" of Earth's subsurface.\n","a- refraction\n","b-isolated.\n","c-reflected.\n","d-refracted\n","\n","من هو النبي الذي كان يملك ملك لم يملكه بقية البشر؟\n","أ- **سليمان عليه السلام**\n","ب- عيسى عليه السلام\n","ج- يحيى عليه السلام\n","د- داود عليه السلام\n","\n","3-Types of seismic waves...(c)\n","a-Body waves\n","b-surface waves\n","c-A&B\n","\n","3-Types of seismic waves... (c)\n","a-Body waves\n","b-surface waves\n","c-A&B\n","\n","4. What is HTML means in techniqal acpact?\n","A- it means CSS\n","B- it actually HTML\n","C- it means HyperText Markup Language\n","D- there is no such thing as HTML\n","\n","109) what is <b> tag refare in HTML? (A)\n","A) bold\n","B) Italic\n","\n","423. What is HTML means in techniqal acpact?\n","A- it means CSS\n","B- it actually HTML\n","D- there is no such thing as HTML\n","\n","1. Which of the following was the capital of the Lydians?\n","A) **Olympia**\n","B) **Miletus**\n","C) **Aspendos**\n","D) **Sardis**\n","\n","The color of tree is green and brown. True\n","\n","1. Which of the following was the capital of the Lydians?\n","A) **Olympia**\n","B) [highlight]Miletus[/highlight]\n","C) **Aspendos**\n","D) Sardis\n","\n","4. What is HTML means in techniqal acpact?\n","A- it means CSS\n","B- it actually HTML\n","C- it means HyperText Markup Language\n","D- there is no such thing as HTML\n","Answer: B and C are the correct answer\n","\"\"\""],"metadata":{"id":"c8bCK2jIQiQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MCQ = []\n","messages = trim_all(messages)\n","def generate_resp(messages):\n","  for message in messages:\n","    messagex = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"\\n\".join([\n","                \"You are a quiz parser. Output JSON for questions, \\\"SKIP\\\" for non-questions.\",\n","                \"\",\n","                \"**For valid multiple-choice questions:**\",\n","                \"- Output JSON with format: {\\\"question\\\": \\\"text\\\", \\\"answers\\\": {\\\"1\\\": \\\"opt1\\\", \\\"2\\\": \\\"opt2\\\"}, \\\"correct\\\": \\\"1\\\"}\",\n","                \"- \\\"correct\\\" must be a string number matching an answer key, or \\\"0\\\" if uncertain.\",\n","                \"\",\n","                \"**Rules for parsing:**\",\n","                \"1. Extract only the question stem and options. Remove answer indicators (stars, bold, 'correct', etc.)\",\n","                \"2. Number answer options sequentially from \\\"1\\\" regardless of original labeling (a, b, 1, 2, etc.)\",\n","                \"3. If ANY uncertainty about correct answer → \\\"correct\\\": \\\"0\\\"\",\n","                \"4. If question indicates correct answer within text → remove indication but parse as normal\",\n","                \"5. If no answer is clearly marked correct → \\\"correct\\\": \\\"0\\\"\",\n","                \"6. If multiple answers appear marked → \\\"correct\\\": \\\"0\\\"\",\n","                \"7. If format is not multiple-choice (no discrete options) → \\\"SKIP\\\"\",\n","                \"8. If it's not a question (statement, explanation, etc.) → \\\"SKIP\\\"\",\n","                \"9. If the input does not contain any multiple-choice options, output \\\"SKIP\\\".\",\n","                \"10. Preserve all original formatting including tabs (\\\\t), spaces, and line breaks. Do not modify whitespace.\",\n","                \"\",\n","                \"**Critical:** If it is not a valid question, output \\\"SKIP\\\".\",\n","                \"**Critical:** NEVER guess or infer correct answers. Only use explicit markings from the input.\",\n","                \"\"\n","            ])\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": message\n","        }\n","    ]\n","    text = tokenizer.apply_chat_template(\n","        messagex,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    generated_ids = model.generate(\n","        model_inputs.input_ids,\n","        max_new_tokens=1024,\n","        do_sample=False, top_k=None, temperature=None, top_p=None,\n","    )\n","\n","    generated_ids = [\n","        output_ids[len(input_ids):]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    response = parse_json(response)\n","    if response and response != \"SKIP\":\n","      if isinstance(response, list):\n","          MCQ.extend(response)\n","      else:\n","          MCQ.append(response)\n","\n","if messages is not None:\n","  generate_resp(messages)"],"metadata":{"id":"SJSZAtsaPQRS","executionInfo":{"status":"ok","timestamp":1767035420721,"user_tz":-180,"elapsed":106266,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60bcde4b-010b-48cc-da87-59b97505ab2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]}]},{"cell_type":"code","source":["json_output = json.dumps(MCQ, indent=2, ensure_ascii=False)\n","print(json_output)"],"metadata":{"id":"ofKpaEpZfSMk","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1767035424806,"user_tz":-180,"elapsed":19,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"0f8177aa-644a-46d8-edae-bd9cf8df6a47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[\n","  {\n","    \"question\": \"What is the value of π (pi) approximately equal to?\",\n","    \"answers\": {\n","      \"1\": \"2.14\",\n","      \"2\": \"3.14\",\n","      \"3\": \"4.13\",\n","      \"4\": \"3.41\"\n","    },\n","    \"correct\": \"2\"\n","  },\n","  {\n","    \"question\": \"Who wrote \\\"Romeo and Juliet\\\"?\",\n","    \"answers\": {\n","      \"1\": \"Charles Dickens\",\n","      \"2\": \"William Shakespeare\",\n","      \"3\": \"George Orwell\",\n","      \"4\": \"Mark Twain\"\n","    },\n","    \"correct\": \"1\"\n","  },\n","  {\n","    \"question\": \"The square root of 144 is:\",\n","    \"answers\": {\n","      \"1\": \"10\",\n","      \"2\": \"11\",\n","      \"3\": \"12\",\n","      \"4\": \"14\"\n","    },\n","    \"correct\": \"3\"\n","  },\n","  {\n","    \"question\": \"After World War I, in European countries, in the Soviet Union and Japan powerfuldictatorships emerged.\",\n","    \"answers\": {\n","      \"1\": \"True\",\n","      \"2\": \"False\"\n","    },\n","    \"correct\": \"1\"\n","  },\n","  {\n","    \"question\": \"promised a better future to Germans that were feeling humiliated by losing theWorld War I.\",\n","    \"answers\": {\n","      \"1\": \"Benito Musoolini\",\n","      \"2\": \"Adolph Hitler\",\n","      \"3\": \"Nicholas II\",\n","      \"4\": \"Peter the Great\"\n","    },\n","    \"correct\": \"2\"\n","  },\n","  {\n","    \"question\": \"What did Voltaire not support?\",\n","    \"answers\": {\n","      \"1\": \"freedom of religion\",\n","      \"2\": \"freedom of expression\",\n","      \"3\": \"separation of church and state.\",\n","      \"4\": \"State control over the economy\"\n","    },\n","    \"correct\": \"4\"\n","  },\n","  {\n","    \"question\": \"..... was the leader during the radical phase of the Fench Revelation.\",\n","    \"answers\": {\n","      \"1\": \"louis 14\",\n","      \"2\": \"Danton\",\n","      \"3\": \"henry 8\",\n","      \"4\": \"Maximilian Robespierre\"\n","    },\n","    \"correct\": \"1\"\n","  },\n","  {\n","    \"question\": \"which of the following was among the consequences of World War III\",\n","    \"answers\": {\n","      \"1\": \"Germany was divided into four regions of administration.\",\n","      \"2\": \"East Germany became under the Soviet control.\",\n","      \"3\": \"the world was divided in two a bi-polar system as Eastern and Western Bloc\",\n","      \"4\": \"All of answers: A , B& C\"\n","    },\n","    \"correct\": \"4\"\n","  },\n","  {\n","    \"question\": \"Which of the following was the capital of the Lydians?\",\n","    \"answers\": {\n","      \"1\": \"Olympia\",\n","      \"2\": \"Miletus\",\n","      \"3\": \"Aspendos\",\n","      \"4\": \"Sardis\"\n","    },\n","    \"correct\": \"1\"\n","  },\n","  {\n","    \"question\": \"Seismic survey use ...C.. sound waves to produce a a \\\"CATScan\\\" of Earth's subsurface.\",\n","    \"answers\": {\n","      \"1\": \"refraction\",\n","      \"2\": \"isolated.\",\n","      \"3\": \"reflected.\",\n","      \"4\": \"refracted\"\n","    },\n","    \"correct\": \"0\"\n","  },\n","  {\n","    \"question\": \"من هو النبي الذي كان يملك ملك لم يملكه بقية البشر؟\",\n","    \"answers\": {\n","      \"1\": \"أ- سليمان عليه السلام\",\n","      \"2\": \"ب- عيسى عليه السلام\",\n","      \"3\": \"ج- يحيى عليه السلام\",\n","      \"4\": \"د- داود عليه السلام\"\n","    },\n","    \"correct\": \"1\"\n","  },\n","  {\n","    \"question\": \"3-Types of seismic waves.....\",\n","    \"answers\": {\n","      \"1\": \"Body waves\",\n","      \"2\": \"surface waves\",\n","      \"3\": \"A&B\"\n","    },\n","    \"correct\": \"3\"\n","  },\n","  {\n","    \"question\": \"Types of seismic waves...\",\n","    \"answers\": {\n","      \"1\": \"Body waves\",\n","      \"2\": \"surface waves\",\n","      \"3\": \"A&B\"\n","    },\n","    \"correct\": \"3\"\n","  },\n","  {\n","    \"question\": \"What is HTML means in techniqal acpact?\",\n","    \"answers\": {\n","      \"1\": \"it means CSS\",\n","      \"2\": \"it actually HTML\",\n","      \"3\": \"it means HyperText Markup Language\",\n","      \"4\": \"there is no such thing as HTML\"\n","    },\n","    \"correct\": \"0\"\n","  },\n","  {\n","    \"question\": \"what is <b> tag refare in HTML?\",\n","    \"answers\": {\n","      \"1\": \"bold\",\n","      \"2\": \"Italic\"\n","    },\n","    \"correct\": \"1\"\n","  },\n","  {\n","    \"question\": \"What is HTML means in techniqal acpact?\",\n","    \"answers\": {\n","      \"1\": \"it means CSS\",\n","      \"2\": \"it actually HTML\",\n","      \"3\": \"there is no such thing as HTML\"\n","    },\n","    \"correct\": \"2\"\n","  },\n","  {\n","    \"question\": \"Which of the following was the capital of the Lydians?\",\n","    \"answers\": {\n","      \"1\": \"Olympia\",\n","      \"2\": \"Miletus\",\n","      \"3\": \"Aspendos\",\n","      \"4\": \"Sardis\"\n","    },\n","    \"correct\": \"2\"\n","  },\n","  {\n","    \"question\": \"The color of tree is green and brown.\",\n","    \"answers\": {\n","      \"1\": \"True\",\n","      \"2\": \"False\"\n","    },\n","    \"correct\": \"1\"\n","  },\n","  {\n","    \"question\": \"Which of the following was the capital of the Lydians?\",\n","    \"answers\": {\n","      \"1\": \"Olympia\",\n","      \"2\": \"Miletus\",\n","      \"3\": \"Aspendos\",\n","      \"4\": \"Sardis\"\n","    },\n","    \"correct\": \"2\"\n","  },\n","  {\n","    \"question\": \"What is HTML means in techniqal acpact?\",\n","    \"answers\": {\n","      \"1\": \"it means CSS\",\n","      \"2\": \"it actually HTML\",\n","      \"3\": \"it means HyperText Markup Language\",\n","      \"4\": \"there is no such thing as HTML\"\n","    },\n","    \"correct\": \"2\"\n","  }\n","]\n"]}]},{"cell_type":"markdown","source":["#Convert Pytorch into GGUF"],"metadata":{"id":"5leiaeGYXCYY"}},{"cell_type":"markdown","source":["###Step 1: Meraging"],"metadata":{"id":"0IOOc49viPff"}},{"cell_type":"code","source":["!pip install transformers peft accelerate bitsandbytes\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","import torch\n","\n","adapter_path = \"/content/drive/MyDrive/llm-finetuning/quiz_parser/checkpoint-414\"\n","output_dir = \"/content/drive/MyDrive/llm-finetuning/quiz_parser_merged\"\n","GGUF_F16 = \"/content/drive/MyDrive/llm-finetuning/quiz_parser_f16.gguf\"\n","GGUF_Q4 = \"/content/drive/MyDrive/llm-finetuning/quiz_parser_q4.gguf\"\n","\n","base_model = \"Qwen/Qwen2.5-1.5B-Instruct\"\n"],"metadata":{"id":"uBRZ2VCyZzFt","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1767096112086,"user_tz":-180,"elapsed":52040,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"0179900c-fc49-4a27-bd6d-247554dfa858"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n","Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n","Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.49.0\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["fp16 = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    low_cpu_mem_usage=True,\n","    trust_remote_code=True\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"],"metadata":{"id":"-x5OxfHf5Z_D","colab":{"base_uri":"https://localhost:8080/","height":383,"referenced_widgets":["ea6221e7ad0e4403bb12de5670cce30a","27196c2ae5334e868de91bb4f10e18ec","3c20bd6bc3cf433f8009f3c7f1e01d33","8570c653db6641649ce8127e342ddae0","a3b0bbeac1cd41d18046d519129c889b","2d1c3d3a5a8a445fa7cb95d1e804182c","173a6ef3839e4abf8bdfeb13cdc49cb1","249dcdc38ed04e09b5405eb434b10cb6","c3ebf038284a4e2f82d3771952ffec14","8953546b3b424b0d9a18ddd5f586c5d7","e083cebee05f425989a0303f156a196b","2e73e9058fc34186b62e0047f5225d43","843eb284feea4254b88a628d007f56a6","06ec36c6b8f54e69843dfb2e4a3dce8d","5aec1132043d4189952d26e0c4ce63ce","ba43ec0f141c4e5686f8fc7ba9e18df4","951c46e2a9ff46eebbf97ebea18e5d14","9c8ea27fcfc34915a54cf83722a560b4","1dac9d1662f743538c2c2c786e98d646","bfd15bbdf87b4db08b6011bb3be7dfee","fedc64eff2c342948c265529536846ff","becdf82aafc340e2bf86a5f42ec4d77d","3d028ffcf39d4543913963cc53a8e0e7","0caa065420bd43ecbf4324f826c540dd","d123b09ce77e49839dda70ecd99d8e37","e306fab2fcf34c3b99fd65ea887e1818","7ec7b0b6636b41c18a62e15e1dbc91cd","00a06bb76f2d4532ace348ef43fc0896","09a6679daca94b339428475dd55c3e03","18492eb43ec243b59762bd00d04bd4d8","a69025dd45d746d9a6edfd31b6b9a956","c87a87bbda084bd2a6e3ce70853be7f2","d9dd421d46db4c3791086148c381bf1d","815084211b044da9886ee6d1287def14","61fcd95d9dcb4ab29dcc5ca227e98b3f","4e5d6a0f327146a3aac1d8264f423873","1c1eb59536dc49a9b34c532534f3f90a","ee19783c7854453db392218e5e377b83","0615da7bbad64e62a831095697776fa9","6bc28a9741e34b8ab7a140f2da6b9193","36beb6ea11914d088c15672974edef10","6bb3878ed3d14cf790845cf9cf6c26e5","e95a8b78094b4b65a19db9ac5f4c4220","797979f086624897a23a530f727cec89","aaa0d58cb5854ee2a111bd6705bcb389","a5b7e18bc05c4965a4264810115bd9c4","e77648c8e52e4367ad544efebb78c0a8","684f7f21f12d4f1fa2643dbce7287104","4bfc24599bc441f992c6a2f697a733c7","1d510fef0bfd418598f01699446c5f96","b9783e78668f413fb0f0ba3fe6810ed7","0d505e989468451e852957990792b736","c5f9d7c627a54ac6967e0f45ffa22d65","3be56811d9644e9dbfa2476629b52c67","ae2b5c4bc5a447a2ad8a45efbb2a5976","d630c7cf1ef84af4b9f5d8e1fea5e037","73050368f3ce4aa69aee262468b0ce92","16a81735ea164eedbb1efad1a1480c53","f43a51cf247840bca5c11956dc8ec64b","6b92b8d7dd2a44d6bc8e5cef95590cab","a0e16c42f25944bc846be316208c6381","38202fb0ea23473c9473ceac3be7f5b4","210d0d50c82140ae9a15045658bac10c","16914e2c36e6494c8d89ed5f791630ca","415d50d5acea4891a78222a1a8a5a198","a89b724326bf47e8918618adff066412","df29c96a36a54651a5827094f386f7c4","dc91282c8be24794af0e017e69cc6c8d","116fe54bd662405192413e195e2bc12f","5e27eee1a9834993be52f1240131a3fa","ccd21f0dc86241aca076114143a737ee","005cf11513624980ae9c6fb717255fba","cf19f575c2d3489e9ce9e5e48800e272","c661e06faf1743fc9f5af9f78ac9ba06","c3f9a6cfd4984c46abf9221f23d84d72","5f3727d04e3b4ab5b64d2e335158c11f","4812d93fd0ef492d9e865896815f240e"]},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1767096163914,"user_tz":-180,"elapsed":47808,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"08d26f6d-5428-441d-ba49-0f44300fc32f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6221e7ad0e4403bb12de5670cce30a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e73e9058fc34186b62e0047f5225d43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d028ffcf39d4543913963cc53a8e0e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815084211b044da9886ee6d1287def14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa0d58cb5854ee2a111bd6705bcb389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d630c7cf1ef84af4b9f5d8e1fea5e037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df29c96a36a54651a5827094f386f7c4"}},"metadata":{}}]},{"cell_type":"code","source":["model = PeftModel.from_pretrained(fp16, adapter_path)\n","model = model.merge_and_unload()"],"metadata":{"id":"cl0n9zxl5crg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"metadata":{"id":"zvCx9EtVl0HK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767096256343,"user_tz":-180,"elapsed":29723,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"2370772c-9c22-4678-db9e-3b7c1b85aa41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/llm-finetuning/quiz_parser_merged/tokenizer_config.json',\n"," '/content/drive/MyDrive/llm-finetuning/quiz_parser_merged/special_tokens_map.json',\n"," '/content/drive/MyDrive/llm-finetuning/quiz_parser_merged/chat_template.jinja',\n"," '/content/drive/MyDrive/llm-finetuning/quiz_parser_merged/vocab.json',\n"," '/content/drive/MyDrive/llm-finetuning/quiz_parser_merged/merges.txt',\n"," '/content/drive/MyDrive/llm-finetuning/quiz_parser_merged/added_tokens.json',\n"," '/content/drive/MyDrive/llm-finetuning/quiz_parser_merged/tokenizer.json')"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["### Step 2: GGUF Proccess"],"metadata":{"id":"Wig1W6VEvRXt"}},{"cell_type":"code","source":["del model\n","del base_model\n","torch.cuda.empty_cache()"],"metadata":{"id":"7h9pnBEVd_Wu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/ggerganov/llama.cpp.git"],"metadata":{"id":"AdpPKr9gXBf0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767096317324,"user_tz":-180,"elapsed":27727,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"91a2c1b1-1534-438b-fc0b-1a95e0cce8cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 74399, done.\u001b[K\n","remote: Counting objects: 100% (120/120), done.\u001b[K\n","remote: Compressing objects: 100% (86/86), done.\u001b[K\n","remote: Total 74399 (delta 71), reused 35 (delta 34), pack-reused 74279 (from 3)\u001b[K\n","Receiving objects: 100% (74399/74399), 270.00 MiB | 27.53 MiB/s, done.\n","Resolving deltas: 100% (53907/53907), done.\n"]}]},{"cell_type":"code","source":["#For GPU (T4)\n","!cd /content/llama.cpp && rm -rf build && cmake -B build -S . -DGGML_CUDA=ON && cmake --build build -j4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"bSlTRNe9FQt8","executionInfo":{"status":"ok","timestamp":1767100977186,"user_tz":-180,"elapsed":2887484,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"20467735-85ca-47ea-d81b-f9153e853509"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-- The C compiler identification is GNU 11.4.0\n","-- The CXX compiler identification is GNU 11.4.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n","-- Found Git: /usr/bin/git (found version \"2.34.1\")\n","-- The ASM compiler identification is GNU\n","-- Found assembler: /usr/bin/cc\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","-- Found Threads: TRUE\n","-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n","-- CMAKE_SYSTEM_PROCESSOR: x86_64\n","-- GGML_SYSTEM_ARCH: x86\n","-- Including CPU backend\n","-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n","-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n","-- Found OpenMP: TRUE (found version \"4.5\")\n","-- x86 detected\n","-- Adding CPU backend variant ggml-cpu: -march=native \n","-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n","-- CUDA Toolkit found\n","-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n","-- Detecting CUDA compiler ABI info\n","-- Detecting CUDA compiler ABI info - done\n","-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","-- Detecting CUDA compile features\n","-- Detecting CUDA compile features - done\n","-- Using CMAKE_CUDA_ARCHITECTURES=75-real CMAKE_CUDA_ARCHITECTURES_NATIVE=75-real\n","-- CUDA host compiler is GNU 11.4.0\n","-- Including CUDA backend\n","-- ggml version: 0.9.4\n","-- ggml commit:  d77d7c5c0\n","-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n","-- Configuring done (9.2s)\n","-- Generating done (0.4s)\n","-- Build files have been written to: /content/llama.cpp/build\n","[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n","[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n","[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n","[  2%] Built target build_info\n","[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n","[  3%] Built target sha256\n","[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n","[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n","[  4%] Built target sha1\n","[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n","[  5%] Built target llama-llava-cli\n","[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  5%] Built target xxhash\n","[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n","[  5%] Built target llama-gemma3-cli\n","[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n","[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n","[  5%] Built target llama-minicpmv-cli\n","[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n","[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n","[  6%] Built target llama-qwen2vl-cli\n","[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n","[  6%] Built target ggml-base\n","[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n","[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\u001b[0m\n","[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n","[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n","[  9%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n","[  9%] Built target cpp-httplib\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n","[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n","[ 10%] Built target ggml-cpu\n","[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o\u001b[0m\n","[ 33%] \u001b[32m\u001b[1mLinking CUDA shared library ../../../bin/libggml-cuda.so\u001b[0m\n","[ 33%] Built target ggml-cuda\n","[ 33%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n","[ 33%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n","[ 33%] Built target ggml\n","[ 33%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n","[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n","[ 34%] Built target llama-gguf\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n","[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n","[ 34%] Built target llama-gguf-hash\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n","[ 58%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n","[ 58%] Built target llama\n","[ 58%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n","[ 59%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n","[ 59%] Built target test-c\n","[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n","[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n","[ 60%] Built target llama-simple\n","[ 60%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n","[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n","[ 60%] Built target llama-simple-chat\n","[ 60%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n","[ 64%] Built target mtmd\n","[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n","[ 66%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n","[ 66%] Built target common\n","[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n","[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n","[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n","[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n","[ 68%] Built target test-tokenizer-0\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n","[ 69%] Built target test-sampling\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n","[ 69%] Built target test-grammar-parser\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n","[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n","[ 69%] Built target test-llama-grammar\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n","[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n","[ 70%] Built target test-grammar-integration\n","[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n","[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n","[ 70%] Built target test-quantize-stats\n","[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n","[ 71%] Built target test-gbnf-validator\n","[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n","[ 71%] Built target test-tokenizer-1-bpe\n","[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n","[ 71%] Built target test-tokenizer-1-spm\n","[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n","[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n","[ 72%] Built target test-json-schema-to-grammar\n","[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n","[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n","[ 72%] Built target test-chat-template\n","[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n","[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n","[ 73%] Built target test-json-partial\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n","[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n","[ 73%] Built target test-log\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n","[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n","[ 73%] Built target test-chat-parser\n","[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n","[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n","[ 74%] Built target test-regex-partial\n","[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n","[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n","[ 74%] Built target test-thread-safety\n","[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n","[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n","[ 75%] Built target test-arg-parser\n","[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n","[ 77%] Built target test-chat-peg-parser\n","[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n","[ 77%] Built target test-opt\n","[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n","[ 77%] Built target test-gguf\n","[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n","[ 78%] Built target test-model-load-cancel\n","[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n","[ 78%] Built target test-autorelease\n","[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n","[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n","[ 80%] Built target test-state-restore-fragmented\n","[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n","[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n","[ 80%] Built target test-barrier\n","[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n","[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n","[ 80%] Built target test-quantize-fns\n","[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n","[ 81%] Built target test-chat\n","[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n","[ 81%] Built target test-quantize-perf\n","[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n","[ 81%] Built target test-rope\n","[ 81%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n","[ 82%] Built target test-mtmd-c-api\n","[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n","[ 82%] Built target test-alloc\n","[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n","[ 83%] Built target llama-batched\n","[ 83%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n","[ 83%] Built target llama-embedding\n","[ 83%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n","[ 83%] Built target test-peg-parser\n","[ 83%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n","[ 84%] Built target llama-idle\n","[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n","[ 85%] Built target llama-eval-callback\n","[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n","[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n","[ 85%] Built target llama-lookahead\n","[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n","[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n","[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n","[ 85%] Built target llama-lookup-create\n","[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n","[ 85%] Built target llama-lookup\n","[ 85%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n","[ 86%] Built target llama-lookup-merge\n","[ 86%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n","[ 86%] Built target llama-lookup-stats\n","[ 86%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n","[ 86%] Built target test-backend-ops\n","[ 86%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n","[ 86%] Built target llama-passkey\n","[ 86%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n","[ 86%] Built target llama-parallel\n","[ 87%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n","[ 87%] Built target llama-save-load-state\n","[ 87%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n","[ 87%] Built target llama-speculative-simple\n","[ 87%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n","[ 87%] Built target llama-retrieval\n","[ 88%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n","[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n","[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n","[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n","[ 88%] Built target llama-speculative\n","[ 88%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n","[ 88%] Built target llama-gen-docs\n","[ 88%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n","[ 88%] Built target llama-finetune\n","[ 89%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n","[ 89%] Built target llama-logits\n","[ 89%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n","[ 89%] Built target llama-vdot\n","[ 89%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n","[ 89%] Built target llama-q8dot\n","[ 89%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n","[ 89%] Built target llama-diffusion-cli\n","[ 89%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n","[ 89%] Built target llama-convert-llama2c-to-ggml\n","[ 89%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n","[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n","[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n","[ 91%] Built target llama-gguf-split\n","[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n","[ 91%] Built target llama-batched-bench\n","[ 92%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n","[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n","[ 92%] Built target llama-imatrix\n","[ 93%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n","[ 93%] Built target llama-bench\n","[ 94%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n","[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n","[ 94%] Built target llama-completion\n","[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n","[ 94%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n","[ 94%] Built target llama-perplexity\n","[ 95%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n","[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n","[ 96%] Built target llama-quantize\n","[ 96%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n","[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n","[ 96%] Built target llama-tokenize\n","[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n","[ 97%] Built target llama-mtmd-cli\n","[ 97%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n","[ 97%] Built target llama-run\n","[ 97%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n","[ 97%] Built target llama-cvector-generator\n","[ 98%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n","[ 98%] Built target llama-export-lora\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n","[ 98%] Built target llama-fit-params\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n","[ 98%] Built target llama-tts\n","[ 98%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n","[ 98%] Built target server-context\n","[ 99%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n","[ 98%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n","[ 99%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n","[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n","[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n","[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n","[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n","[ 99%] Built target llama-cli\n","[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-task.cpp.o\u001b[0m\n","[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-queue.cpp.o\u001b[0m\n","[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-common.cpp.o\u001b[0m\n","[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-context.cpp.o\u001b[0m\n","[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n","[100%] Built target llama-server\n"]}]},{"cell_type":"code","source":["#For cpu\n","!apt-get update\n","!apt-get install -y build-essential cmake python3-pip\n","\n","!cd /content/llama.cpp && \\\n","rm -rf build && \\\n","cmake -B build -S . && \\\n","cmake --build build -j2"],"metadata":{"id":"BliO7aH9vMtd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -r /content/llama.cpp/requirements.txt"],"metadata":{"id":"7wLJy2xYvNUY","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1767101082633,"user_tz":-180,"elapsed":101316,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"e2370b63-1c6a-4ef9-c26c-ea344bcaf663"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.6/178.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.6.0+cpu which is incompatible.\n","torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.6.0+cpu which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!python /content/llama.cpp/convert_hf_to_gguf.py \\\n","    {output_dir} \\\n","    --outfile {GGUF_F16} \\\n","    --outtype f16"],"metadata":{"id":"iF2kEXOHvdhF","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1767101145466,"user_tz":-180,"elapsed":58745,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"32b600da-56f9-4b91-cf63-70bf721d040d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:hf-to-gguf:Loading model: quiz_parser_merged\n","INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n","INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {1536, 151936}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.float16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {1536, 256}\n","INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 32768\n","INFO:hf-to-gguf:gguf: embedding length = 1536\n","INFO:hf-to-gguf:gguf: feed forward length = 8960\n","INFO:hf-to-gguf:gguf: head count = 12\n","INFO:hf-to-gguf:gguf: key-value head count = 2\n","INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n","INFO:hf-to-gguf:gguf: file type = 1\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","The tokenizer you are loading from '/content/drive/MyDrive/llm-finetuning/quiz_parser_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n","INFO:gguf.vocab:Adding 151387 merge(s).\n","INFO:gguf.vocab:Setting special token type eos to 151645\n","INFO:gguf.vocab:Setting special token type pad to 151643\n","INFO:gguf.vocab:Setting special token type bos to 151643\n","INFO:gguf.vocab:Setting add_bos_token to False\n","INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n","    {{- '<|im_start|>system\\n' }}\n","    {%- if messages[0]['role'] == 'system' %}\n","        {{- messages[0]['content'] }}\n","    {%- else %}\n","        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n","    {%- endif %}\n","    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n","    {%- for tool in tools %}\n","        {{- \"\\n\" }}\n","        {{- tool | tojson }}\n","    {%- endfor %}\n","    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n","{%- else %}\n","    {%- if messages[0]['role'] == 'system' %}\n","        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n","    {%- else %}\n","        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n","    {%- endif %}\n","{%- endif %}\n","{%- for message in messages %}\n","    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n","        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n","    {%- elif message.role == \"assistant\" %}\n","        {{- '<|im_start|>' + message.role }}\n","        {%- if message.content %}\n","            {{- '\\n' + message.content }}\n","        {%- endif %}\n","        {%- for tool_call in message.tool_calls %}\n","            {%- if tool_call.function is defined %}\n","                {%- set tool_call = tool_call.function %}\n","            {%- endif %}\n","            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n","            {{- tool_call.name }}\n","            {{- '\", \"arguments\": ' }}\n","            {{- tool_call.arguments | tojson }}\n","            {{- '}\\n</tool_call>' }}\n","        {%- endfor %}\n","        {{- '<|im_end|>\\n' }}\n","    {%- elif message.role == \"tool\" %}\n","        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n","            {{- '<|im_start|>user' }}\n","        {%- endif %}\n","        {{- '\\n<tool_response>\\n' }}\n","        {{- message.content }}\n","        {{- '\\n</tool_response>' }}\n","        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n","            {{- '<|im_end|>\\n' }}\n","        {%- endif %}\n","    {%- endif %}\n","{%- endfor %}\n","{%- if add_generation_prompt %}\n","    {{- '<|im_start|>assistant\\n' }}\n","{%- endif %}\n","\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/drive/MyDrive/llm-finetuning/quiz_parser_f16.gguf: n_tensors = 338, total_size = 3.1G\n","Writing: 100% 3.09G/3.09G [00:37<00:00, 83.1Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/drive/MyDrive/llm-finetuning/quiz_parser_f16.gguf\n"]}]},{"cell_type":"markdown","source":["###Q4_K_M (optional)"],"metadata":{"id":"0zFc43EhwdOW"}},{"cell_type":"code","source":["!cd llama.cpp && \\\n","./build/bin/llama-quantize {GGUF_F16} {GGUF_Q4} Q4_K_M"],"metadata":{"id":"gyHuvQ-jwhzS","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1767101370088,"user_tz":-180,"elapsed":163138,"user":{"displayName":"Fadhl Alfadhili","userId":"11322104510017051223"}},"outputId":"3e25bf31-5ec5-4a9c-b41f-38dfc4767086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["main: build = 7579 (d77d7c5c0)\n","main: built with GNU 11.4.0 for Linux x86_64\n","main: quantizing '/content/drive/MyDrive/llm-finetuning/quiz_parser_f16.gguf' to '/content/drive/MyDrive/llm-finetuning/quiz_parser_q4.gguf' as Q4_K_M\n","llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from /content/drive/MyDrive/llm-finetuning/quiz_parser_f16.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n","llama_model_loader: - kv   1:                               general.type str              = model\n","llama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\n","llama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.800000\n","llama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.700000\n","llama_model_loader: - kv   5:                               general.name str              = Quiz_Parser_Merged\n","llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n","llama_model_loader: - kv   7:                          qwen2.block_count u32              = 28\n","llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n","llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 1536\n","llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 8960\n","llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 12\n","llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n","llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  15:                          general.file_type u32              = 1\n","llama_model_loader: - kv  16:               general.quantization_version u32              = 2\n","llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n","llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n","llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n","llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\n","llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\n","llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\n","llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n","llama_model_loader: - type  f32:  141 tensors\n","llama_model_loader: - type  f16:  197 tensors\n","ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n","ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n","ggml_cuda_init: found 1 CUDA devices:\n","  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n","[   1/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[   2/ 338]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n","[   3/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[   4/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[   5/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[   6/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[   7/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[   8/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[   9/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  10/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  11/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  12/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  13/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  14/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  15/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  16/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  17/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  18/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  19/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  20/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  21/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  22/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  23/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  24/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  25/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  26/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  27/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  28/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  29/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  30/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  31/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  32/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  33/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  34/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  35/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  36/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  37/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  38/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  39/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  40/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  41/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  42/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  43/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  44/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  45/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  46/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  47/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  48/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  49/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  50/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  51/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  52/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  53/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  54/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  55/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  56/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  57/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  58/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  59/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  60/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  61/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  62/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  63/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  64/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  65/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  66/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  67/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  68/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  69/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  70/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  71/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  72/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  73/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  74/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  75/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  76/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  77/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  78/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  79/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  80/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  81/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  82/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  83/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  84/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  85/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  86/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  87/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  88/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  89/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  90/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  91/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  92/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[  93/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  94/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[  95/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  96/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  97/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  98/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[  99/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 100/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 101/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 102/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 103/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 104/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 105/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 106/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 107/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 108/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 109/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 110/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 111/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 112/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 113/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 114/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 115/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 116/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 117/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 118/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 119/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 120/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 121/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 122/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 123/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 124/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 125/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 126/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 127/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 128/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 129/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 130/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 131/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 132/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 133/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 134/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 135/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 136/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 137/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 138/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 139/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 140/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 141/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 142/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 143/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 144/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 145/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 146/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 147/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 148/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 149/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 150/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 151/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 152/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 153/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 154/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 155/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 156/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 157/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 158/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 159/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 160/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 161/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 162/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 163/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 164/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 165/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 166/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 167/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 168/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 169/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 170/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 171/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 172/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 173/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 174/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 175/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 176/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 177/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 178/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 179/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 180/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 181/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 182/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 183/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 184/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 185/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 186/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 187/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 188/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 189/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 190/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 191/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 192/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 193/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 194/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 195/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 196/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 197/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 198/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 199/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 200/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 201/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 202/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 203/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 204/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 205/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 206/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 207/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 208/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 209/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 210/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 211/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 212/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 213/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 214/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 215/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 216/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 217/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 218/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 219/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 220/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 221/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 222/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 223/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 224/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 225/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 226/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 227/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 228/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 229/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 230/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 231/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 232/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 233/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 234/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 235/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 236/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 237/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 238/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 239/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 240/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 241/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 242/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 243/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 244/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 245/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 246/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 247/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 248/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 249/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 250/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 251/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 252/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 253/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 254/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 255/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 256/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 257/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 258/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 259/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 260/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 261/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 262/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 263/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 264/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 265/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 266/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 267/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 268/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 269/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 270/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 271/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 272/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 273/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 274/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 275/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 276/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 277/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 278/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 279/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 280/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 281/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 282/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 283/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 284/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 285/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 286/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 287/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 288/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 289/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 290/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 291/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 292/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 293/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 294/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 295/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 296/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 297/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 298/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 299/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 300/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 301/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 302/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 303/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 304/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 305/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 306/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 307/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 308/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 309/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 310/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 311/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 312/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 313/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 314/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 315/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 316/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 317/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 318/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 319/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 320/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 321/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 322/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 323/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 324/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 325/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 326/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 327/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 328/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n","[ 329/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 330/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 331/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 332/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n","[ 333/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 334/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 335/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 336/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","[ 337/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 338/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n","llama_model_quantize_impl: model size  =  2944.68 MiB\n","llama_model_quantize_impl: quant size  =   934.69 MiB\n","\n","main: quantize time = 161513.09 ms\n","main:    total time = 161513.09 ms\n"]}]}]}